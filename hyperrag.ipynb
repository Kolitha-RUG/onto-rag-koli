{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "046ff76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.5.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: rdflib in d:\\rag\\onto-rag-koli\\venv\\lib\\site-packages (7.1.4)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0.post1-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pandas in d:\\rag\\onto-rag-koli\\venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in d:\\rag\\onto-rag-koli\\venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: rapidfuzz in d:\\rag\\onto-rag-koli\\venv\\lib\\site-packages (3.13.0)\n",
      "Collecting pyshacl\n",
      "  Downloading pyshacl-0.30.1-py3-none-any.whl.metadata (35 kB)\n",
      "Collecting httpx>=0.27 (from ollama)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic>=2.9 (from ollama)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in d:\\rag\\onto-rag-koli\\venv\\lib\\site-packages (from rdflib) (3.2.3)\n",
      "Requirement already satisfied: packaging in d:\\rag\\onto-rag-koli\\venv\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\rag\\onto-rag-koli\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\rag\\onto-rag-koli\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\rag\\onto-rag-koli\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Collecting owlrl<8,>=7.1.2 (from pyshacl)\n",
      "  Downloading owlrl-7.1.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting prettytable>=3.7.0 (from pyshacl)\n",
      "  Downloading prettytable-3.16.0-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting html5rdf<2,>=1.2 (from rdflib[html]!=7.1.2,<8.0,>=7.1.1->pyshacl)\n",
      "  Downloading html5rdf-1.2.1-py2.py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting anyio (from httpx>=0.27->ollama)\n",
      "  Using cached anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: certifi in d:\\rag\\onto-rag-koli\\venv\\lib\\site-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
      "Collecting httpcore==1.* (from httpx>=0.27->ollama)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in d:\\rag\\onto-rag-koli\\venv\\lib\\site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.27->ollama)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: wcwidth in d:\\rag\\onto-rag-koli\\venv\\lib\\site-packages (from prettytable>=3.7.0->pyshacl) (0.2.13)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->ollama)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.9->ollama)\n",
      "  Using cached pydantic_core-2.33.2-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic>=2.9->ollama)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9->ollama)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\rag\\onto-rag-koli\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.27->ollama)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Downloading faiss_cpu-1.11.0.post1-cp313-cp313-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 11.0/14.9 MB 56.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 52.3 MB/s eta 0:00:00\n",
      "Downloading pyshacl-0.30.1-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 46.8 MB/s eta 0:00:00\n",
      "Downloading owlrl-7.1.4-py3-none-any.whl (51 kB)\n",
      "Downloading html5rdf-1.2.1-py2.py3-none-any.whl (109 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading prettytable-3.16.0-py3-none-any.whl (33 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: typing-extensions, sniffio, prettytable, networkx, html5rdf, h11, faiss-cpu, annotated-types, typing-inspection, pydantic-core, owlrl, httpcore, anyio, pyshacl, pydantic, httpx, ollama\n",
      "\n",
      "   -- -------------------------------------  1/17 [sniffio]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   ------- --------------------------------  3/17 [networkx]\n",
      "   --------- ------------------------------  4/17 [html5rdf]\n",
      "   --------- ------------------------------  4/17 [html5rdf]\n",
      "   --------- ------------------------------  4/17 [html5rdf]\n",
      "   --------- ------------------------------  4/17 [html5rdf]\n",
      "   ----------- ----------------------------  5/17 [h11]\n",
      "   -------------- -------------------------  6/17 [faiss-cpu]\n",
      "   -------------- -------------------------  6/17 [faiss-cpu]\n",
      "   -------------- -------------------------  6/17 [faiss-cpu]\n",
      "   -------------- -------------------------  6/17 [faiss-cpu]\n",
      "   -------------- -------------------------  6/17 [faiss-cpu]\n",
      "   --------------------- ------------------  9/17 [pydantic-core]\n",
      "   ----------------------- ---------------- 10/17 [owlrl]\n",
      "   ------------------------- -------------- 11/17 [httpcore]\n",
      "   ------------------------- -------------- 11/17 [httpcore]\n",
      "   ---------------------------- ----------- 12/17 [anyio]\n",
      "   ---------------------------- ----------- 12/17 [anyio]\n",
      "   ---------------------------- ----------- 12/17 [anyio]\n",
      "   ---------------------------- ----------- 12/17 [anyio]\n",
      "   ------------------------------ --------- 13/17 [pyshacl]\n",
      "   ------------------------------ --------- 13/17 [pyshacl]\n",
      "   ------------------------------ --------- 13/17 [pyshacl]\n",
      "   ------------------------------ --------- 13/17 [pyshacl]\n",
      "   ------------------------------ --------- 13/17 [pyshacl]\n",
      "   -------------------------------- ------- 14/17 [pydantic]\n",
      "   -------------------------------- ------- 14/17 [pydantic]\n",
      "   -------------------------------- ------- 14/17 [pydantic]\n",
      "   -------------------------------- ------- 14/17 [pydantic]\n",
      "   -------------------------------- ------- 14/17 [pydantic]\n",
      "   -------------------------------- ------- 14/17 [pydantic]\n",
      "   -------------------------------- ------- 14/17 [pydantic]\n",
      "   -------------------------------- ------- 14/17 [pydantic]\n",
      "   -------------------------------- ------- 14/17 [pydantic]\n",
      "   ----------------------------------- ---- 15/17 [httpx]\n",
      "   ----------------------------------- ---- 15/17 [httpx]\n",
      "   ---------------------------------------- 17/17 [ollama]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.10.0 faiss-cpu-1.11.0.post1 h11-0.16.0 html5rdf-1.2.1 httpcore-1.0.9 httpx-0.28.1 networkx-3.5 ollama-0.5.3 owlrl-7.1.4 prettytable-3.16.0 pydantic-2.11.7 pydantic-core-2.33.2 pyshacl-0.30.1 sniffio-1.3.1 typing-extensions-4.14.1 typing-inspection-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama rdflib networkx faiss-cpu pandas numpy rapidfuzz pyshacl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013bd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 1 — Setup\n",
    "# If needed, uncomment:\n",
    "# !pip install ollama rdflib networkx faiss-cpu pandas numpy rapidfuzz pyshacl\n",
    "\n",
    "import os, re, json, time, uuid, glob\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ollama\n",
    "import rdflib\n",
    "from rdflib import Graph\n",
    "from rdflib.namespace import RDF, RDFS, OWL, SKOS, XSD\n",
    "import networkx as nx\n",
    "import faiss\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    na = np.linalg.norm(a) + 1e-9\n",
    "    nb = np.linalg.norm(b) + 1e-9\n",
    "    return float((a @ b) / (na * nb))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "140050fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM: mistral | EMBED: nomic-embed-text\n",
      "Data: D:\\RAG\\onto-rag-koli\\data | Output: D:\\RAG\\onto-rag-koli\\outputs\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Config\n",
    "DATA_DIR = Path(\"data\")\n",
    "ONTOLOGY_TTL = DATA_DIR / \"sws.ttl\"\n",
    "CORPUS_DIR = DATA_DIR / \"corpus\"\n",
    "\n",
    "LLM_MODEL   = os.environ.get(\"OLLAMA_LLM\", \"mistral\")\n",
    "EMBED_MODEL = os.environ.get(\"OLLAMA_EMBED\", \"nomic-embed-text\")\n",
    "\n",
    "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Retrieval / alignment knobs\n",
    "K_ENTITY = 60\n",
    "K_HYPEREDGE = 60\n",
    "ENTITY_ALIGN_MIN = 0.55\n",
    "NEW_CONCEPT_FREQ_MIN = 2\n",
    "\n",
    "DATA_DIR.mkdir(exist_ok=True); CORPUS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"LLM:\", LLM_MODEL, \"| EMBED:\", EMBED_MODEL)\n",
    "print(\"Data:\", DATA_DIR.resolve(), \"| Output:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa38e550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama models: [None, None, None, None, None, None, None, None]\n",
      "Note: pull chat model ->  ollama pull mistral\n",
      "Note: pull embed model -> ollama pull nomic-embed-text\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — Check Ollama\n",
    "def check_ollama():\n",
    "    try:\n",
    "        ms = ollama.list()\n",
    "        names = [m.get(\"name\") for m in ms.get(\"models\", [])]\n",
    "        print(\"Ollama models:\", names)\n",
    "        if LLM_MODEL not in names:\n",
    "            print(f\"Note: pull chat model ->  ollama pull {LLM_MODEL}\")\n",
    "        if EMBED_MODEL not in names:\n",
    "            print(f\"Note: pull embed model -> ollama pull {EMBED_MODEL}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"Could not reach Ollama. Run: `ollama serve`\")\n",
    "        raise\n",
    "_ = check_ollama()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff19cfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classes=71, obj_props=26, dt_props=13\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — Load upper ontology (TTL → catalogs)\n",
    "g = Graph(); g.parse(str(ONTOLOGY_TTL))\n",
    "RDFS = rdflib.namespace.RDFS; OWL = rdflib.namespace.OWL; SKOS = rdflib.namespace.SKOS\n",
    "\n",
    "def lit(s): return str(s) if s is not None else \"\"\n",
    "\n",
    "def label(graph, node):\n",
    "    for l in graph.objects(node, RDFS.label): return lit(l)\n",
    "    return None\n",
    "\n",
    "def alts(graph, node):\n",
    "    return [lit(a) for a in graph.objects(node, SKOS.altLabel)]\n",
    "\n",
    "classes = [{\"iri\": str(c), \"label\": label(g,c) or str(c), \"alt\": alts(g,c)}\n",
    "           for c in g.subjects(rdflib.RDF.type, OWL.Class)]\n",
    "\n",
    "obj_props = [{\"iri\": str(p), \"label\": label(g,p) or str(p), \"alt\": alts(g,p)} \n",
    "             for p in g.subjects(rdflib.RDF.type, OWL.ObjectProperty)]\n",
    "\n",
    "dt_props  = [{\"iri\": str(p), \"label\": label(g,p) or str(p), \"alt\": alts(g,p)} \n",
    "             for p in g.subjects(rdflib.RDF.type, OWL.DatatypeProperty)]\n",
    "\n",
    "print(f\"Loaded classes={len(classes)}, obj_props={len(obj_props)}, dt_props={len(dt_props)}\")\n",
    "ONTO = {\"classes\": classes, \"obj_props\": obj_props, \"dt_props\": dt_props}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96896062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 1\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Load Markdown corpus\n",
    "\n",
    "# Drop a demo file if empty:\n",
    "if not any(CORPUS_DIR.glob(\"*.md\")):\n",
    "    print(\"No docs found\")\n",
    "    \n",
    "DOCS = [{\"path\": str(p), \"text\": p.read_text(encoding=\"utf-8\", errors=\"ignore\")}\n",
    "        for p in sorted(CORPUS_DIR.glob(\"*.md\"))]\n",
    "print(\"Docs:\", len(DOCS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa6b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import json5\n",
    "!pip install json5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1282b0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperedges per doc: [197]\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — N-ary extraction (hyperedges + entities) via Ollama\n",
    "EXTRACTION_PROMPT_TMPL = Template(\"\"\"\n",
    "You extract n-ary relational facts (hyperedges) and entities.\n",
    "\n",
    "Return ONE JSON object ONLY. No prose, no extra blocks, no code fences.\n",
    "\n",
    "{\n",
    "  \"hyperedges\": [\n",
    "    {\n",
    "      \"text\": \"...\",\n",
    "      \"score\": 0.0,\n",
    "      \"entities\": [\n",
    "        { \"name\": \"...\", \"type\": \"...\", \"description\": \"...\", \"key_score\": 0.0 }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "- Split the input into coherent knowledge fragments (as hyperedges).\n",
    "- Include all entities per hyperedge.\n",
    "- Keep scores as floats.\n",
    "- Same language as the input.\n",
    "\n",
    "INPUT:\n",
    "---\n",
    "$chunk\n",
    "---\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def _strip_code_fences(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"```\"):\n",
    "        # remove leading and trailing fenced blocks\n",
    "        s = s.strip(\"`\")\n",
    "        # often starts with ```json\n",
    "        if s.lower().startswith(\"json\"):\n",
    "            s = s[4:].lstrip()\n",
    "    return s\n",
    "\n",
    "def _clean_common_json_issues(s: str) -> str:\n",
    "    # normalize smart quotes\n",
    "    s = s.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"’\", \"'\")\n",
    "    # remove trailing commas before } or ]\n",
    "    s = re.sub(r',\\s*([}\\]])', r'\\1', s)\n",
    "    # remove BOM if present\n",
    "    s = s.lstrip(\"\\ufeff\")\n",
    "    return s\n",
    "\n",
    "def _extract_json_objects(s: str) -> list[str]:\n",
    "    \"\"\"Extract top-level {...} blocks from a string that may contain multiple JSON objects.\"\"\"\n",
    "    objs, in_str, esc, depth, start = [], False, False, 0, None\n",
    "    for i, ch in enumerate(s):\n",
    "        if in_str:\n",
    "            if esc:\n",
    "                esc = False\n",
    "            elif ch == '\\\\':\n",
    "                esc = True\n",
    "            elif ch == '\"':\n",
    "                in_str = False\n",
    "            continue\n",
    "        else:\n",
    "            if ch == '\"':\n",
    "                in_str = True\n",
    "            elif ch == '{':\n",
    "                if depth == 0:\n",
    "                    start = i\n",
    "                depth += 1\n",
    "            elif ch == '}':\n",
    "                depth -= 1\n",
    "                if depth == 0 and start is not None:\n",
    "                    objs.append(s[start:i+1])\n",
    "                    start = None\n",
    "    return objs\n",
    "\n",
    "def _try_load_json(s: str):\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        if json5 is not None:\n",
    "            try:\n",
    "                return json5.loads(s)\n",
    "            except Exception:\n",
    "                pass\n",
    "    # final attempt: clean again then try std json\n",
    "    s2 = _clean_common_json_issues(s)\n",
    "    try:\n",
    "        return json.loads(s2)\n",
    "    except Exception:\n",
    "        if json5 is not None:\n",
    "            try:\n",
    "                return json5.loads(s2)\n",
    "            except Exception:\n",
    "                pass\n",
    "    raise\n",
    "\n",
    "def ollama_json(prompt: str, model=LLM_MODEL, tries=2) -> dict:\n",
    "    last_err = None\n",
    "    for _ in range(tries + 1):\n",
    "        # Prefer strict JSON mode; some models may ignore it\n",
    "        try:\n",
    "            resp = ollama.generate(\n",
    "                model=model,\n",
    "                prompt=prompt,\n",
    "                options={\"temperature\": 0.2, \"format\": \"json\"}\n",
    "            )\n",
    "        except Exception:\n",
    "            resp = ollama.generate(model=model, prompt=prompt, options={\"temperature\": 0.2})\n",
    "\n",
    "        raw = resp.get(\"response\", \"\").strip()\n",
    "        txt = _strip_code_fences(raw)\n",
    "        txt = _clean_common_json_issues(txt)\n",
    "\n",
    "        # 1) If it’s a single JSON object, parse directly\n",
    "        try:\n",
    "            obj = _try_load_json(txt)\n",
    "            if isinstance(obj, dict) and \"hyperedges\" in obj:\n",
    "                return obj\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "\n",
    "        # 2) Otherwise, merge multiple top-level JSON objects\n",
    "        merged = {\"hyperedges\": []}\n",
    "        ok = False\n",
    "        for chunk in _extract_json_objects(txt):\n",
    "            try:\n",
    "                one = _try_load_json(_clean_common_json_issues(chunk))\n",
    "                if isinstance(one, dict) and \"hyperedges\" in one and isinstance(one[\"hyperedges\"], list):\n",
    "                    merged[\"hyperedges\"].extend(one[\"hyperedges\"])\n",
    "                    ok = True\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "        if ok:\n",
    "            return merged\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    # Debug help: write the raw text so you can inspect the failing case\n",
    "    dbg_path = OUT_DIR / \"failed_ollama_response.txt\"\n",
    "    dbg_path.write_text(raw, encoding=\"utf-8\")\n",
    "    raise RuntimeError(f\"Failed to parse JSON from model output. Last error: {last_err}. \"\n",
    "                       f\"Saved raw response to: {dbg_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def extract_hypergraph(text: str, max_chars=2500):\n",
    "    # simple line-based chunking\n",
    "    lines, chunks, buf, n = text.splitlines(), [], [], 0\n",
    "    for L in lines:\n",
    "        if n + len(L) + 1 > max_chars and buf:\n",
    "            chunks.append(\"\\n\".join(buf)); buf=[]; n=0\n",
    "        buf.append(L); n += len(L)+1\n",
    "    if buf: chunks.append(\"\\n\".join(buf))\n",
    "\n",
    "    out = []\n",
    "    for ch in chunks:\n",
    "        prompt = EXTRACTION_PROMPT_TMPL.substitute(chunk=ch)\n",
    "        obj = ollama_json(prompt)\n",
    "        for he in obj.get(\"hyperedges\", []):\n",
    "            he[\"id\"] = str(uuid.uuid4())\n",
    "            for ent in he.get(\"entities\", []):\n",
    "                ent[\"id\"] = str(uuid.uuid4())\n",
    "            out.append(he)\n",
    "    return out\n",
    "\n",
    "# cache\n",
    "CACHE = OUT_DIR / \"extraction.json\"\n",
    "if CACHE.exists():\n",
    "    data = json.loads(CACHE.read_text())\n",
    "else:\n",
    "    data = {\"docs\":[]}\n",
    "    for d in DOCS:\n",
    "        hyperedges = extract_hypergraph(d[\"text\"])\n",
    "        data[\"docs\"].append({\"path\": d[\"path\"], \"hyperedges\": hyperedges})\n",
    "    CACHE.write_text(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"Hyperedges per doc:\", [len(d[\"hyperedges\"]) for d in data[\"docs\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f148f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities=620 Hyperedges=197 Edges=620\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — Build bipartite hypergraph + embeddings + FAISS\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "ENTITY_NODES, HYPER_NODES = {}, {}\n",
    "\n",
    "def add_hyperedge(doc_path, he):\n",
    "    hid = he[\"id\"]\n",
    "    HYPER_NODES[hid] = {\"id\": hid, \"text\": he.get(\"text\",\"\"), \"score\": float(he.get(\"score\",0)), \"doc\": doc_path}\n",
    "    G.add_node(hid, kind=\"hyperedge\")\n",
    "    for ent in he.get(\"entities\", []):\n",
    "        eid = ent[\"id\"]\n",
    "        if eid not in ENTITY_NODES:\n",
    "            ENTITY_NODES[eid] = {\"id\": eid, \"name\": ent.get(\"name\",\"\"), \"type\": ent.get(\"type\",\"Entity\"),\n",
    "                                 \"description\": ent.get(\"description\",\"\"), \"key_score\": float(ent.get(\"key_score\",0))}\n",
    "            G.add_node(eid, kind=\"entity\")\n",
    "        G.add_edge(hid, eid, kind=\"MENTIONS\")\n",
    "\n",
    "for doc in data[\"docs\"]:\n",
    "    for he in doc[\"hyperedges\"]:\n",
    "        add_hyperedge(doc[\"path\"], he)\n",
    "\n",
    "print(f\"Entities={len(ENTITY_NODES)} Hyperedges={len(HYPER_NODES)} Edges={G.number_of_edges()}\")\n",
    "\n",
    "def embed_texts(texts: List[str], model=EMBED_MODEL):\n",
    "    vecs=[]\n",
    "    for t in texts:\n",
    "        r = ollama.embeddings(model=model, prompt=t)\n",
    "        vecs.append(np.array(r[\"embedding\"], dtype=np.float32))\n",
    "    return np.vstack(vecs) if vecs else np.zeros((0,768), dtype=np.float32)\n",
    "\n",
    "entity_ids = list(ENTITY_NODES.keys())\n",
    "hyper_ids  = list(HYPER_NODES.keys())\n",
    "\n",
    "E_TEXTS = [ENTITY_NODES[e][\"name\"] + \" :: \" + ENTITY_NODES[e][\"description\"] for e in entity_ids]\n",
    "H_TEXTS = [HYPER_NODES[h][\"text\"] for h in hyper_ids]\n",
    "\n",
    "E_MAT = embed_texts(E_TEXTS)\n",
    "H_MAT = embed_texts(H_TEXTS)\n",
    "\n",
    "def build_index(mat):\n",
    "    if mat.size == 0: return None\n",
    "    d = mat.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    matn = mat / (np.linalg.norm(mat,axis=1,keepdims=True)+1e-9)\n",
    "    index.add(matn.astype(np.float32))\n",
    "    return index\n",
    "\n",
    "E_INDEX = build_index(E_MAT)\n",
    "H_INDEX = build_index(H_MAT)\n",
    "\n",
    "def embed_query(q:str):\n",
    "    r = ollama.embeddings(model=EMBED_MODEL, prompt=q)\n",
    "    return np.array(r[\"embedding\"], dtype=np.float32)\n",
    "\n",
    "def faiss_search(index, mat, qvec, k=10):\n",
    "    qn = qvec / (np.linalg.norm(qvec)+1e-9)\n",
    "    D, I = index.search(qn[np.newaxis,:].astype(np.float32), k)\n",
    "    return D[0], I[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4eb16a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 15\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — Dual retrieval + expansion\n",
    "def retrieve_entities(query:str, k=K_ENTITY):\n",
    "    if E_INDEX is None: return []\n",
    "    qv = embed_query(query)\n",
    "    D,I = faiss_search(E_INDEX, E_MAT, qv, k=min(k, len(entity_ids)))\n",
    "    out=[]\n",
    "    for d,i in zip(D,I):\n",
    "        if i<0: continue\n",
    "        eid = entity_ids[int(i)]\n",
    "        out.append({\"id\": eid, \"score\": float(d), **ENTITY_NODES[eid]})\n",
    "    return out\n",
    "\n",
    "def retrieve_hyperedges(query:str, k=K_HYPEREDGE):\n",
    "    if H_INDEX is None: return []\n",
    "    qv = embed_query(query)\n",
    "    D,I = faiss_search(H_INDEX, H_MAT, qv, k=min(k, len(hyper_ids)))\n",
    "    out=[]\n",
    "    for d,i in zip(D,I):\n",
    "        if i<0: continue\n",
    "        hid = hyper_ids[int(i)]\n",
    "        out.append({\"id\": hid, \"score\": float(d), **HYPER_NODES[hid]})\n",
    "    return out\n",
    "\n",
    "def expand_from_entities(eids: List[str]):\n",
    "    facts=[]\n",
    "    for eid in eids:\n",
    "        for nbr in G.neighbors(eid):\n",
    "            if G.nodes[nbr].get(\"kind\")==\"hyperedge\":\n",
    "                ents=[n for n in G.neighbors(nbr) if G.nodes[n].get(\"kind\")==\"entity\"]\n",
    "                facts.append({\"hyperedge\": HYPER_NODES[nbr], \"entities\":[ENTITY_NODES[x] for x in ents]})\n",
    "    return facts\n",
    "\n",
    "def expand_from_hyperedges(hids: List[str]):\n",
    "    facts=[]\n",
    "    for hid in hids:\n",
    "        ents=[n for n in G.neighbors(hid) if G.nodes[n].get(\"kind\")==\"entity\"]\n",
    "        facts.append({\"hyperedge\": HYPER_NODES[hid], \"entities\":[ENTITY_NODES[x] for x in ents]})\n",
    "    return facts\n",
    "\n",
    "def fused_retrieval(query:str, k_entity=K_ENTITY, k_hyper=K_HYPEREDGE):\n",
    "    ents = retrieve_entities(query, k_entity)\n",
    "    hyps = retrieve_hyperedges(query, k_hyper)\n",
    "    eids = [e[\"id\"] for e in ents]\n",
    "    hids = [h[\"id\"] for h in hyps]\n",
    "    facts = expand_from_entities(eids) + expand_from_hyperedges(hids)\n",
    "    uniq, out = set(), []\n",
    "    for f in facts:\n",
    "        hid = f[\"hyperedge\"][\"id\"]\n",
    "        if hid not in uniq:\n",
    "            uniq.add(hid); out.append(f)\n",
    "    return ents, hyps, out\n",
    "\n",
    "# smoke test\n",
    "e,h,f = fused_retrieval(\"hypertension creatinine male mild elevation\", 10, 10)\n",
    "print(len(e), len(h), len(f))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0ac9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 — Prepare ontology label embeddings\n",
    "def flatten_labels(items):\n",
    "    out=[]\n",
    "    for it in items:\n",
    "        variants = [it.get(\"label\")] + it.get(\"alt\", [])\n",
    "        variants = [v for v in variants if v]\n",
    "        text = \" || \".join(dict.fromkeys(variants)) if variants else it[\"iri\"]\n",
    "        out.append({\"iri\": it[\"iri\"], \"text\": text})\n",
    "    return out\n",
    "\n",
    "CLASS_LABELS = flatten_labels(ONTO[\"classes\"])\n",
    "OBJPROP_LABELS = flatten_labels(ONTO[\"obj_props\"])\n",
    "DTPROP_LABELS  = flatten_labels(ONTO[\"dt_props\"])\n",
    "\n",
    "def embed_list_texts(rows): \n",
    "    return embed_texts([r[\"text\"] for r in rows]) if rows else np.zeros((0,0), dtype=np.float32)\n",
    "\n",
    "CLS_MAT = embed_list_texts(CLASS_LABELS)\n",
    "OP_MAT  = embed_list_texts(OBJPROP_LABELS)\n",
    "DP_MAT  = embed_list_texts(DTPROP_LABELS)\n",
    "\n",
    "def build_idx(mat):\n",
    "    if mat.size==0: return None\n",
    "    idx = faiss.IndexFlatIP(mat.shape[1])\n",
    "    idx.add((mat/(np.linalg.norm(mat,axis=1,keepdims=True)+1e-9)).astype(np.float32))\n",
    "    return idx\n",
    "\n",
    "CLS_IDX = build_idx(CLS_MAT)\n",
    "OP_IDX  = build_idx(OP_MAT)\n",
    "DP_IDX  = build_idx(DP_MAT)\n",
    "\n",
    "def nearest(mat, idx, qvec, k=5):\n",
    "    if idx is None or mat.size==0: return []\n",
    "    qn = qvec / (np.linalg.norm(qvec)+1e-9)\n",
    "    D,I = idx.search(qn[np.newaxis,:].astype(np.float32), k)\n",
    "    return list(zip(D[0].tolist(), I[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7eb2a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Alignment helpers\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def lexical_sim(a:str, b:str)->float:\n",
    "    if not a or not b: return 0.0\n",
    "    return 0.01 * fuzz.token_set_ratio(a.lower(), b.lower())  # 0..1\n",
    "\n",
    "def align_entity_to_class(ent:Dict[str,Any], topk=5, alpha=0.65):\n",
    "    text = (ent.get(\"name\",\"\") + \" :: \" + ent.get(\"description\",\"\")).strip()\n",
    "    qv = embed_query(text)\n",
    "    nns = nearest(CLS_MAT, CLS_IDX, qv, k=topk)\n",
    "    out=[]\n",
    "    for score, i in nns:\n",
    "        cls = CLASS_LABELS[int(i)]\n",
    "        lex = lexical_sim(text, cls[\"text\"])\n",
    "        comb = float(alpha*score + (1-alpha)*lex)\n",
    "        out.append({\"class_iri\": cls[\"iri\"], \"class_text\": cls[\"text\"], \"embed\": float(score), \"lex\": float(lex), \"score\": comb})\n",
    "    return sorted(out, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "def suggest_property(ent_type_or_label:str, prefer_dt=False, topk=3):\n",
    "    if prefer_dt:\n",
    "        pool, mat, idx = DTPROP_LABELS, DP_MAT, DP_IDX\n",
    "    else:\n",
    "        pool, mat, idx = OBJPROP_LABELS, OP_MAT, OP_IDX\n",
    "    if mat.size==0 or idx is None: return []\n",
    "    qv = embed_query(ent_type_or_label)\n",
    "    nns = nearest(mat, idx, qv, k=topk)\n",
    "    return [{\"prop_iri\": pool[i][\"iri\"], \"prop_text\": pool[i][\"text\"], \"embed\": float(s)} for s,i in nns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "979b2854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":Situation_be2f0654 a :ClinicalSituation .\n",
      ":9552f7e6_Industry_5_0_paradigm a <http://example.org/onto#Industry_5_0_paradigm> ; rdfs:label \"Industry 5.0 paradigm\" .\n",
      ":c207a65b_DTs a <http://www.sws.org/sws#DigitalTwin> ; rdfs:label \"DTs\" .\n",
      ":15a3699d_human_actor a <http://www.sws.org/sws#Actor> ; rdfs:label \"human actor\" .\n",
      ":be7e7596_physical_parameters a <http://example.org/onto#physical_parameters>  \n",
      "---\n",
      " :Industry_5_0_paradigm a owl:Class ; rdfs:label \"Industry 5.0 paradigm\" .\n",
      ":physical_parameters a owl:Class ; rdfs:label \"physical parameters\" .\n"
     ]
    }
   ],
   "source": [
    "def iri_safe(s:str)->str:\n",
    "    return re.sub(r'[^A-Za-z0-9_]+', '_', s).strip('_') or (\"X_\"+uuid.uuid4().hex[:6])\n",
    "\n",
    "BASE = \"http://example.org/onto#\"\n",
    "\n",
    "def propose_axioms_for_fact(fact:Dict[str,Any], cls_threshold=ENTITY_ALIGN_MIN):\n",
    "    he = fact[\"hyperedge\"]; ents = fact[\"entities\"]\n",
    "    situation_id = \"Situation_\" + he[\"id\"].split(\"-\")[0]\n",
    "    abox, tbox, new_classes = [], [], []\n",
    "\n",
    "    abox.append(f\":{situation_id} a :ClinicalSituation .\")\n",
    "\n",
    "    bindings=[]\n",
    "    for e in ents:\n",
    "        label = e[\"name\"] or e[\"type\"]\n",
    "        eid = f\"{e['id'].split('-')[0]}_{iri_safe(label)[:24]}\"\n",
    "        best = align_entity_to_class(e, topk=5)\n",
    "        if best and best[0][\"score\"] >= cls_threshold:\n",
    "            cls_iri = best[0][\"class_iri\"]\n",
    "        else:\n",
    "            new_cls = f\":{iri_safe(label)}\"\n",
    "            tbox.append(f\"{new_cls} a owl:Class ; rdfs:label \\\"{label}\\\" .\")\n",
    "            cls_iri = f\"{BASE}{iri_safe(label)}\"\n",
    "            new_classes.append({\"proposed\": cls_iri, \"label\": label})\n",
    "        abox.append(f\":{eid} a <{cls_iri}> ; rdfs:label \\\"{label}\\\" .\")\n",
    "        bindings.append({\"id\": eid, \"label\": label, \"type\": e.get(\"type\",\"\")})\n",
    "\n",
    "    for b in bindings:\n",
    "        prefer_dt = \"measure\" in (b[\"type\"] or b[\"label\"]).lower()\n",
    "        props = suggest_property(b[\"type\"] or b[\"label\"], prefer_dt=prefer_dt, topk=1)\n",
    "        if props:\n",
    "            piri = props[0][\"prop_iri\"]\n",
    "        else:\n",
    "            piri = f\"{BASE}relatedTo\"\n",
    "            tbox.append(f\"<{piri}> a owl:ObjectProperty ; rdfs:label \\\"related to\\\" .\")\n",
    "        abox.append(f\":{situation_id} <{piri}> :{b['id']} .\")\n",
    "\n",
    "    abox.append(f\":{situation_id} rdfs:comment \\\"{he['text'].replace('\\\"','\\\\\\\"')}\\\" .\")\n",
    "    return {\"abox\":\"\\n\".join(abox), \"tbox\":\"\\n\".join(sorted(set(tbox))), \"situation\": situation_id, \"new_classes\": new_classes}\n",
    "\n",
    "# Try on first fused fact\n",
    "_,_,facts = fused_retrieval(\"hypertension creatinine mild male\", 10, 10)\n",
    "if facts:\n",
    "    demo = propose_axioms_for_fact(facts[0])\n",
    "    print(demo[\"abox\"][:400], \"\\n---\\n\", demo[\"tbox\"][:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5108629c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>freq</th>\n",
       "      <th>best_align</th>\n",
       "      <th>parent_iri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI4Work</td>\n",
       "      <td>7</td>\n",
       "      <td>0.392436</td>\n",
       "      <td>http://www.sws.org/sws#Task</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI4Work components</td>\n",
       "      <td>7</td>\n",
       "      <td>0.418128</td>\n",
       "      <td>http://www.sws.org/sws#HumanActor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI</td>\n",
       "      <td>7</td>\n",
       "      <td>0.510613</td>\n",
       "      <td>http://www.sws.org/sws#AIAcceptance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SWS Management</td>\n",
       "      <td>5</td>\n",
       "      <td>0.449268</td>\n",
       "      <td>http://www.sws.org/sws#Operational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HDT</td>\n",
       "      <td>4</td>\n",
       "      <td>0.425655</td>\n",
       "      <td>http://www.sws.org/sws#HardConstraint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>virtual assistant</td>\n",
       "      <td>4</td>\n",
       "      <td>0.450721</td>\n",
       "      <td>http://www.sws.org/sws#HumanActor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SWS Management component</td>\n",
       "      <td>4</td>\n",
       "      <td>0.461684</td>\n",
       "      <td>http://www.sws.org/sws#Task</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>manufacturing</td>\n",
       "      <td>4</td>\n",
       "      <td>0.474152</td>\n",
       "      <td>http://www.sws.org/sws#Productivity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Operators</td>\n",
       "      <td>4</td>\n",
       "      <td>0.498976</td>\n",
       "      <td>http://www.sws.org/sws#Operational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>human</td>\n",
       "      <td>4</td>\n",
       "      <td>0.519578</td>\n",
       "      <td>http://www.sws.org/sws#HumanActor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label  freq  best_align  \\\n",
       "0                   AI4Work     7    0.392436   \n",
       "1        AI4Work components     7    0.418128   \n",
       "2                        AI     7    0.510613   \n",
       "3            SWS Management     5    0.449268   \n",
       "4                       HDT     4    0.425655   \n",
       "5         virtual assistant     4    0.450721   \n",
       "6  SWS Management component     4    0.461684   \n",
       "7             manufacturing     4    0.474152   \n",
       "8                 Operators     4    0.498976   \n",
       "9                     human     4    0.519578   \n",
       "\n",
       "                              parent_iri  \n",
       "0            http://www.sws.org/sws#Task  \n",
       "1      http://www.sws.org/sws#HumanActor  \n",
       "2    http://www.sws.org/sws#AIAcceptance  \n",
       "3     http://www.sws.org/sws#Operational  \n",
       "4  http://www.sws.org/sws#HardConstraint  \n",
       "5      http://www.sws.org/sws#HumanActor  \n",
       "6            http://www.sws.org/sws#Task  \n",
       "7    http://www.sws.org/sws#Productivity  \n",
       "8     http://www.sws.org/sws#Operational  \n",
       "9      http://www.sws.org/sws#HumanActor  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def best_align_score(e): \n",
    "    a = align_entity_to_class(e, topk=3)\n",
    "    return a[0][\"score\"] if a else 0.0\n",
    "\n",
    "def discover_new_concepts(freq_min=NEW_CONCEPT_FREQ_MIN, score_max=ENTITY_ALIGN_MIN):\n",
    "    counter, ents_by_name = Counter(), {}\n",
    "    for h in HYPER_NODES.values():\n",
    "        for eid in [n for n in G.neighbors(h[\"id\"]) if G.nodes[n][\"kind\"]==\"entity\"]:\n",
    "            ent = ENTITY_NODES[eid]; key = (ent[\"name\"] or ent[\"type\"]).strip()\n",
    "            ents_by_name[key] = ent; counter[key]+=1\n",
    "    proposals=[]\n",
    "    for name, freq in counter.items():\n",
    "        e = ents_by_name[name]; s = best_align_score(e)\n",
    "        if freq >= freq_min and s < score_max:\n",
    "            qv = embed_query(name); nns = nearest(CLS_MAT, CLS_IDX, qv, k=1)\n",
    "            parent = CLASS_LABELS[nns[0][1]][\"iri\"] if nns else f\"{BASE}Entity\"\n",
    "            proposals.append({\"label\": name, \"freq\": freq, \"best_align\": s, \"parent_iri\": parent})\n",
    "    return sorted(proposals, key=lambda x:(-x[\"freq\"], x[\"best_align\"]))\n",
    "\n",
    "NEW_CONCEPTS = discover_new_concepts()\n",
    "pd.DataFrame(NEW_CONCEPTS)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acfc008f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: D:\\RAG\\onto-rag-koli\\outputs\\abox.ttl\n",
      "Wrote: D:\\RAG\\onto-rag-koli\\outputs\\tbox.ttl\n",
      "Wrote: D:\\RAG\\onto-rag-koli\\outputs\\new_concepts.csv\n"
     ]
    }
   ],
   "source": [
    "def export_updates_for_query(query:str, top_facts=10):\n",
    "    _,_,facts = fused_retrieval(query, K_ENTITY, K_HYPEREDGE)\n",
    "    facts = facts[:top_facts]\n",
    "    abox_all, tbox_all, new_rows = [], [], []\n",
    "    for f in facts:\n",
    "        ax = propose_axioms_for_fact(f)\n",
    "        if ax[\"abox\"]: abox_all.append(ax[\"abox\"])\n",
    "        if ax[\"tbox\"]: tbox_all.append(ax[\"tbox\"])\n",
    "        new_rows += ax[\"new_classes\"]\n",
    "\n",
    "    abox_ttl = \"@prefix : <http://example.org/onto#> .\\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\\n@prefix owl: <http://www.w3.org/2002/07/owl#> .\\n\\n\" + \"\\n\\n\".join(abox_all)\n",
    "    tbox_ttl = \"@prefix : <http://example.org/onto#> .\\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\\n@prefix owl: <http://www.w3.org/2002/07/owl#> .\\n\\n\" + \"\\n\\n\".join(sorted(set(tbox_all)))\n",
    "\n",
    "    (OUT_DIR / \"abox.ttl\").write_text(abox_ttl)\n",
    "    (OUT_DIR / \"tbox.ttl\").write_text(tbox_ttl)\n",
    "    pd.DataFrame(NEW_CONCEPTS).to_csv(OUT_DIR / \"new_concepts.csv\", index=False)\n",
    "    pd.DataFrame(new_rows).to_csv(OUT_DIR / \"new_classes_from_facts.csv\", index=False)\n",
    "    print(\"Wrote:\", (OUT_DIR/\"abox.ttl\").resolve())\n",
    "    print(\"Wrote:\", (OUT_DIR/\"tbox.ttl\").resolve())\n",
    "    print(\"Wrote:\", (OUT_DIR/\"new_concepts.csv\").resolve())\n",
    "\n",
    "export_updates_for_query(\"hypertension serum creatinine male mild elevation\", top_facts=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
