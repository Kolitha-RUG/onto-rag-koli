{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e4a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdflib in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (7.1.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (3.13.0)\n",
      "Requirement already satisfied: requests in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (from rdflib) (3.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (from requests) (2025.7.14)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\deede\\.pyenv\\pyenv-win\\versions\\3.13.5\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install rdflib pandas rapidfuzz requests pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803657c",
   "metadata": {},
   "source": [
    "Cell 0 — Setup (install & imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc1fa732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready.\n"
     ]
    }
   ],
   "source": [
    "# If needed, uncomment to install:\n",
    "# !pip install rdflib pandas rapidfuzz requests pyyaml\n",
    "\n",
    "import os, json, math, re, uuid, time, pathlib, textwrap\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import pandas as pd\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, URIRef\n",
    "from rapidfuzz import process, fuzz\n",
    "import numpy as np\n",
    "\n",
    "print(\"Imports ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ac1aa",
   "metadata": {},
   "source": [
    "Cell 1 — Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7b88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config set. Edit TTL_PATH and DOC_MD_PATH before running.\n"
     ]
    }
   ],
   "source": [
    "# --- REQUIRED: set your file paths here ---\n",
    "TTL_PATH = r\"sws.ttl\"     # e.g., r\"C:\\projects\\base.ttl\"\n",
    "DOC_MD_PATH = r\"swsmd.md\"        # e.g., r\"C:\\projects\\usecase.md\"\n",
    "\n",
    "# Ollama local model (e.g., 'llama3.1:8b', 'qwen2.5:7b', etc.)\n",
    "OLLAMA_MODEL = \"mistral\"\n",
    "\n",
    "# Choose \"flat_json\" (default, simpler) or \"jsonld\"\n",
    "MAPPING_FORMAT = \"flat_json\"\n",
    "\n",
    "# Embedding settings\n",
    "USE_EMBEDDINGS = True                 # turn off to fall back to fuzzy-only\n",
    "OLLAMA_EMBED_MODEL = \"nomic-embed-text\"\n",
    "EMBED_TOPK = 5                        # how many nearest ontology terms to consider\n",
    "EMBED_MIN = 0.75                      # cosine threshold to accept a match directly\n",
    "COMBO_ALPHA = 0.65                    # blend: alpha*cosine + (1-alpha)*fuzzy\n",
    "COMBO_MIN = 0.80                      # accept if blended score >= this\n",
    "EMBED_BATCH = 64                      # batch size for embedding ontology surface forms\n",
    "\n",
    "\n",
    "# Output directory\n",
    "OUT_DIR = \"./outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Config set. Edit TTL_PATH and DOC_MD_PATH before running.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e6f7c",
   "metadata": {},
   "source": [
    "Cell 2 — Load ontology & build lookup index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271143af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71 classes, 26 object properties, 13 data properties.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kind</th>\n",
       "      <th>iri</th>\n",
       "      <th>qname</th>\n",
       "      <th>label</th>\n",
       "      <th>alt_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>class</td>\n",
       "      <td>http://data.europa.eu/esco/model#Skill</td>\n",
       "      <td>esco:Skill</td>\n",
       "      <td>Skill</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>class</td>\n",
       "      <td>http://www.sws.org/sws#AIAcceptance</td>\n",
       "      <td>:AIAcceptance</td>\n",
       "      <td>AIAcceptance</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>class</td>\n",
       "      <td>http://www.sws.org/sws#Accuracy</td>\n",
       "      <td>:Accuracy</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class</td>\n",
       "      <td>http://www.sws.org/sws#Actor</td>\n",
       "      <td>:Actor</td>\n",
       "      <td>Actor</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>class</td>\n",
       "      <td>http://www.sws.org/sws#ActorStatus</td>\n",
       "      <td>:ActorStatus</td>\n",
       "      <td>ActorStatus</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>data_property</td>\n",
       "      <td>http://www.sws.org/sws#hasSkillLevel</td>\n",
       "      <td>:hasSkillLevel</td>\n",
       "      <td>hasSkillLevel</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>data_property</td>\n",
       "      <td>http://www.sws.org/sws#hasSocialPerformanceScore</td>\n",
       "      <td>:hasSocialPerformanceScore</td>\n",
       "      <td>hasSocialPerformanceScore</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>data_property</td>\n",
       "      <td>http://www.sws.org/sws#hasTaskComplexity</td>\n",
       "      <td>:hasTaskComplexity</td>\n",
       "      <td>hasTaskComplexity</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>data_property</td>\n",
       "      <td>http://www.sws.org/sws#hasTaskStatusDescriptor</td>\n",
       "      <td>:hasTaskStatusDescriptor</td>\n",
       "      <td>hasTaskStatusDescriptor</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>data_property</td>\n",
       "      <td>http://www.sws.org/sws#hasTechnicalPerformance...</td>\n",
       "      <td>:hasTechnicalPerformanceScore</td>\n",
       "      <td>hasTechnicalPerformanceScore</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              kind                                                iri  \\\n",
       "0            class             http://data.europa.eu/esco/model#Skill   \n",
       "1            class                http://www.sws.org/sws#AIAcceptance   \n",
       "2            class                    http://www.sws.org/sws#Accuracy   \n",
       "3            class                       http://www.sws.org/sws#Actor   \n",
       "4            class                 http://www.sws.org/sws#ActorStatus   \n",
       "..             ...                                                ...   \n",
       "105  data_property               http://www.sws.org/sws#hasSkillLevel   \n",
       "106  data_property   http://www.sws.org/sws#hasSocialPerformanceScore   \n",
       "107  data_property           http://www.sws.org/sws#hasTaskComplexity   \n",
       "108  data_property     http://www.sws.org/sws#hasTaskStatusDescriptor   \n",
       "109  data_property  http://www.sws.org/sws#hasTechnicalPerformance...   \n",
       "\n",
       "                             qname                         label alt_labels  \n",
       "0                       esco:Skill                         Skill         []  \n",
       "1                    :AIAcceptance                  AIAcceptance         []  \n",
       "2                        :Accuracy                      Accuracy         []  \n",
       "3                           :Actor                         Actor         []  \n",
       "4                     :ActorStatus                   ActorStatus         []  \n",
       "..                             ...                           ...        ...  \n",
       "105                 :hasSkillLevel                 hasSkillLevel         []  \n",
       "106     :hasSocialPerformanceScore     hasSocialPerformanceScore         []  \n",
       "107             :hasTaskComplexity             hasTaskComplexity         []  \n",
       "108       :hasTaskStatusDescriptor       hasTaskStatusDescriptor         []  \n",
       "109  :hasTechnicalPerformanceScore  hasTechnicalPerformanceScore         []  \n",
       "\n",
       "[110 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph()\n",
    "g.parse(TTL_PATH, format=\"turtle\")\n",
    "\n",
    "def qname_safe(uri: URIRef) -> str:\n",
    "    try:\n",
    "        return g.namespace_manager.normalizeUri(uri)\n",
    "    except Exception:\n",
    "        return str(uri)\n",
    "\n",
    "def collect_terms(graph: Graph):\n",
    "    classes = sorted(set(graph.subjects(RDF.type, OWL.Class)))\n",
    "    obj_props = sorted(set(graph.subjects(RDF.type, OWL.ObjectProperty)))\n",
    "    data_props = sorted(set(graph.subjects(RDF.type, OWL.DatatypeProperty)))\n",
    "    return classes, obj_props, data_props\n",
    "\n",
    "def get_label(graph: Graph, term: URIRef) -> str:\n",
    "    lab = graph.value(term, RDFS.label)\n",
    "    return str(lab) if lab else qname_safe(term)\n",
    "\n",
    "classes, obj_props, data_props = collect_terms(g)\n",
    "\n",
    "def build_index(graph: Graph, terms: List[URIRef], kind: str):\n",
    "    rows = []\n",
    "    for t in terms:\n",
    "        row = {\n",
    "            \"kind\": kind,\n",
    "            \"iri\": str(t),\n",
    "            \"qname\": qname_safe(t),\n",
    "            \"label\": get_label(graph, t),\n",
    "            \"alt_labels\": [str(o) for o in graph.objects(t, SKOS.altLabel)]\n",
    "        }\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "rows = []\n",
    "rows += build_index(g, classes, \"class\")\n",
    "rows += build_index(g, obj_props, \"object_property\")\n",
    "rows += build_index(g, data_props, \"data_property\")\n",
    "\n",
    "onto_df = pd.DataFrame(rows).fillna(\"\")\n",
    "print(f\"Loaded {len(classes)} classes, {len(obj_props)} object properties, {len(data_props)} data properties.\")\n",
    "onto_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682ad43",
   "metadata": {},
   "source": [
    "Cell 3 — Build fuzzy matchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b97a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space built with 220 surface forms.\n"
     ]
    }
   ],
   "source": [
    "def normalize(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s.lower().strip())\n",
    "\n",
    "def build_search_space(df: pd.DataFrame):\n",
    "    vocab, key_to_meta = [], {}\n",
    "    for _, row in df.iterrows():\n",
    "        entries = set([row[\"qname\"], row[\"label\"]])\n",
    "        for a in (row[\"alt_labels\"] if isinstance(row[\"alt_labels\"], list) else []):\n",
    "            entries.add(a)\n",
    "        for e in entries:\n",
    "            k = normalize(str(e))\n",
    "            vocab.append(k)\n",
    "            key_to_meta[k] = {\"kind\": row[\"kind\"], \"iri\": row[\"iri\"], \"qname\": row[\"qname\"], \"label\": row[\"label\"]}\n",
    "    return vocab, key_to_meta\n",
    "\n",
    "vocab, key_to_meta = build_search_space(onto_df)\n",
    "print(f\"Search space built with {len(vocab)} surface forms.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ee0283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 220 ontology surface forms with nomic-embed-text ...\n",
      "Ontology embedding index ready: (220, 768)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def _try_embed_batch(texts, model, timeout=120):\n",
    "    \"\"\"Try Ollama embeddings with batched 'input'. Fallback to per-item 'prompt' if needed.\n",
    "    Returns list of vectors (len == len(texts)).\"\"\"\n",
    "    url = \"http://localhost:11434/api/embeddings\"\n",
    "    # Preferred: batched 'input'\n",
    "    try:\n",
    "        r = requests.post(url, json={\"model\": model, \"input\": texts}, timeout=timeout)\n",
    "        if r.ok:\n",
    "            data = r.json()\n",
    "            # Possible shapes:\n",
    "            # 1) { \"embeddings\": [[...], [...], ...] }\n",
    "            # 2) { \"embeddings\": [ { \"embedding\": [...] }, ... ] }\n",
    "            embs = data.get(\"embeddings\")\n",
    "            if isinstance(embs, list) and embs:\n",
    "                if isinstance(embs[0], dict) and \"embedding\" in embs[0]:\n",
    "                    return [e[\"embedding\"] for e in embs]\n",
    "                if isinstance(embs[0], list):\n",
    "                    return embs\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: per-text requests with 'prompt'\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        rr = requests.post(url, json={\"model\": model, \"prompt\": t}, timeout=timeout)\n",
    "        rr.raise_for_status()\n",
    "        dj = rr.json()\n",
    "        vec = dj.get(\"embedding\")\n",
    "        if vec is None:\n",
    "            data = dj.get(\"data\")\n",
    "            if isinstance(data, list) and data and isinstance(data[0], dict):\n",
    "                vec = data[0].get(\"embedding\")\n",
    "        out.append(vec)\n",
    "    return out\n",
    "\n",
    "def embed_texts_ollama(texts, model, batch=64):\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch):\n",
    "        chunk = texts[i:i+batch]\n",
    "        vecs.extend(_try_embed_batch(chunk, model))\n",
    "    return np.array(vecs, dtype=np.float32)\n",
    "\n",
    "def l2_normalize(mat):\n",
    "    mat = np.array(mat, dtype=np.float32)\n",
    "    norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-9\n",
    "    return mat / norms\n",
    "\n",
    "# Build embedding index over ontology surface forms (from Cell 3)\n",
    "if USE_EMBEDDINGS:\n",
    "    print(f\"Embedding {len(vocab)} ontology surface forms with {OLLAMA_EMBED_MODEL} ...\")\n",
    "    E_onto = embed_texts_ollama(vocab, OLLAMA_EMBED_MODEL, batch=EMBED_BATCH)\n",
    "    E_onto = l2_normalize(E_onto)\n",
    "    print(\"Ontology embedding index ready:\", E_onto.shape)\n",
    "else:\n",
    "    E_onto = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da7ec8",
   "metadata": {},
   "source": [
    "Cell 4 — Load Markdown and chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7685e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 22 chunks.\n"
     ]
    }
   ],
   "source": [
    "with open(DOC_MD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    md_text = f.read()\n",
    "\n",
    "CHUNK_CHAR_LIMIT = 2500  # adjust if needed\n",
    "paras = [p.strip() for p in re.split(r\"\\n{2,}\", md_text) if p.strip()]\n",
    "chunks, buf = [], \"\"\n",
    "for p in paras:\n",
    "    if len(buf) + len(p) + 2 <= CHUNK_CHAR_LIMIT:\n",
    "        buf = (buf + \"\\n\\n\" + p).strip()\n",
    "    else:\n",
    "        if buf: chunks.append(buf)\n",
    "        buf = p\n",
    "if buf: chunks.append(buf)\n",
    "\n",
    "print(f\"Document split into {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33923693",
   "metadata": {},
   "source": [
    "Cell 5 — Ollama helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa544d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama helper ready.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def ollama_chat(model: str, system_prompt: str, user_prompt: str, as_json: bool = True, timeout: int = 120):\n",
    "    \"\"\"Calls Ollama's /api/chat endpoint. Ensure Ollama is running locally.\"\"\"\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    hdrs = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        \"options\": {\"temperature\": 0.2}\n",
    "    }\n",
    "    r = requests.post(url, headers=hdrs, json=data, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    content = r.json().get(\"message\", {}).get(\"content\", \"\")\n",
    "    if as_json:\n",
    "        m = re.search(r\"\\{.*\\}\", content, flags=re.DOTALL)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return content\n",
    "\n",
    "print(\"Ollama helper ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35572c45",
   "metadata": {},
   "source": [
    "Cell 6 — Mapping prompts (Flat JSON default, JSON-LD optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c95d4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt templates ready.\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert ontology mapper. Given an ontology dictionary and a text chunk, \"\n",
    "    \"extract: (A) mentions aligned to existing ontology classes/properties, and (B) new concept candidates. \"\n",
    "    \"Return the requested output format exactly.\"\n",
    ")\n",
    "\n",
    "def make_user_prompt(chunk_text: str, ontology_terms: pd.DataFrame, format: str = \"flat_json\") -> str:\n",
    "    MAX_TERMS = 300\n",
    "    df = ontology_terms.head(MAX_TERMS)\n",
    "\n",
    "    lines = []\n",
    "    for _, r in df.iterrows():\n",
    "        alts = \", \".join(r[\"alt_labels\"]) if isinstance(r[\"alt_labels\"], list) else \"\"\n",
    "        base = f\"- [{r['kind']}] {r['qname']} | label: {r['label']}\"\n",
    "        lines.append(base + (f\" | alt: {alts}\" if alts else \"\"))\n",
    "\n",
    "    if format == \"flat_json\":\n",
    "        out_spec = \"\"\"\n",
    "Return strictly valid JSON with two arrays:\n",
    "{\n",
    "  \"mapped\": [\n",
    "    {\n",
    "      \"span\": \"<exact text>\",\n",
    "      \"sentence\": \"<the sentence>\",\n",
    "      \"concept_kind\": \"class|object_property|data_property|instance\",\n",
    "      \"ontology_term\": \"<qname or iri>\",\n",
    "      \"confidence\": 0-1\n",
    "    }\n",
    "  ],\n",
    "  \"new_candidates\": [\n",
    "    {\n",
    "      \"term\": \"<preferred label>\",\n",
    "      \"type_guess\": \"class|object_property|data_property\",\n",
    "      \"definition\": \"<one-sentence definition inferred from text>\",\n",
    "      \"example_sentence\": \"<verbatim sentence from chunk>\",\n",
    "      \"rationale\": \"<why it seems new>\",\n",
    "      \"confidence\": 0-1\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "    else:\n",
    "        out_spec = \"\"\"\n",
    "Return strictly valid JSON-LD with an @graph array; also add top-level \"new_candidates\".\n",
    "Use @type for class and properties as keys; keep nesting minimal.\n",
    "{\n",
    "  \"@graph\": [ { \"@type\": \"ex:YourClass\", \"ex:someProperty\": \"value\" } ],\n",
    "  \"new_candidates\": [ { \"term\": \"...\", \"type_guess\": \"...\", \"definition\": \"...\", \"example_sentence\": \"...\", \"confidence\": 0-1 } ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"ONTOLOGY DICTIONARY (subset):\n",
    "{chr(10).join(lines)}\n",
    "\n",
    "TEXT CHUNK (verbatim):\n",
    "<<<\n",
    "{chunk_text}\n",
    "<<<\n",
    "\n",
    "OUTPUT FORMAT SPEC:\n",
    "{out_spec}\n",
    "\n",
    "Rules:\n",
    "- Use only terms that appear in the dictionary for \"ontology_term\".\n",
    "- If no matching term exists for a mention, put it under \"new_candidates\".\n",
    "- Be conservative; prefer precision over recall.\n",
    "- Ensure the JSON is syntactically valid.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "print(\"Prompt templates ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8525f1",
   "metadata": {},
   "source": [
    "Cell 7 — Run mapping over chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a6e2e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/22 ...\n",
      "Processing chunk 2/22 ...\n",
      "Processing chunk 3/22 ...\n",
      "Processing chunk 4/22 ...\n",
      "Processing chunk 5/22 ...\n",
      "Processing chunk 6/22 ...\n",
      "Processing chunk 7/22 ...\n",
      "Processing chunk 8/22 ...\n",
      "Processing chunk 9/22 ...\n",
      "Processing chunk 10/22 ...\n",
      "Processing chunk 11/22 ...\n",
      "Processing chunk 12/22 ...\n",
      "Processing chunk 13/22 ...\n",
      "Processing chunk 14/22 ...\n",
      "Processing chunk 15/22 ...\n",
      "Processing chunk 16/22 ...\n",
      "Processing chunk 17/22 ...\n",
      "Processing chunk 18/22 ...\n",
      "Processing chunk 19/22 ...\n",
      "Processing chunk 20/22 ...\n",
      "Processing chunk 21/22 ...\n",
      "Processing chunk 22/22 ...\n",
      "Collected 158 mapped mentions and 66 new concept candidates.\n"
     ]
    }
   ],
   "source": [
    "all_mapped, all_new = [], []\n",
    "fmt = \"flat_json\" if MAPPING_FORMAT.lower() == \"flat_json\" else \"jsonld\"\n",
    "\n",
    "for i, ch in enumerate(chunks, 1):\n",
    "    print(f\"Processing chunk {i}/{len(chunks)} ...\")\n",
    "    up = make_user_prompt(ch, onto_df, format=fmt)\n",
    "    resp = ollama_chat(OLLAMA_MODEL, SYSTEM_PROMPT, up, as_json=True)\n",
    "\n",
    "    if isinstance(resp, dict) and \"mapped\" in resp:  # flat_json path\n",
    "        mapped = resp.get(\"mapped\", [])\n",
    "        newc = resp.get(\"new_candidates\", [])\n",
    "    else:  # jsonld path → flatten a bit\n",
    "        mapped = resp.get(\"@graph\", []) if isinstance(resp, dict) else []\n",
    "        newc = resp.get(\"new_candidates\", []) if isinstance(resp, dict) else []\n",
    "        flat_rows = []\n",
    "        for item in mapped:\n",
    "            cls = item.get(\"@type\", \"\")\n",
    "            for k, v in item.items():\n",
    "                if k.startswith(\"@\"):\n",
    "                    continue\n",
    "                if isinstance(v, list):\n",
    "                    for el in v:\n",
    "                        flat_rows.append({\"span\": str(el), \"sentence\": \"\", \"concept_kind\": \"object_property\", \"ontology_term\": k, \"confidence\": 0.6})\n",
    "                else:\n",
    "                    flat_rows.append({\"span\": str(v), \"sentence\": \"\", \"concept_kind\": \"object_property\", \"ontology_term\": k, \"confidence\": 0.6})\n",
    "            if cls:\n",
    "                flat_rows.append({\"span\": str(cls), \"sentence\": \"\", \"concept_kind\": \"class\", \"ontology_term\": str(cls), \"confidence\": 0.6})\n",
    "        mapped = flat_rows\n",
    "\n",
    "    all_mapped.extend(mapped)\n",
    "    all_new.extend(newc)\n",
    "\n",
    "print(f\"Collected {len(all_mapped)} mapped mentions and {len(all_new)} new concept candidates.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab63a82d",
   "metadata": {},
   "source": [
    "Cell 8 — Post-process (align & filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2131659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned mapped mentions: 32\n",
      "New concept candidates: 66\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>sentence</th>\n",
       "      <th>concept_kind</th>\n",
       "      <th>ontology_term</th>\n",
       "      <th>confidence</th>\n",
       "      <th>span_norm</th>\n",
       "      <th>match_meta</th>\n",
       "      <th>matched_qname</th>\n",
       "      <th>matched_kind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Operational Equipment Effectiveness (OEE)</td>\n",
       "      <td>Packaging material production lines are genera...</td>\n",
       "      <td>class</td>\n",
       "      <td>esco:Performance</td>\n",
       "      <td>0.8</td>\n",
       "      <td>operational equipment effectiveness (oee)</td>\n",
       "      <td>{'qname': ':Operational', 'kind': 'class'}</td>\n",
       "      <td>:Operational</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>less skilled, inexperienced operators</td>\n",
       "      <td>Given the repetitive and demanding nature of m...</td>\n",
       "      <td>instance</td>\n",
       "      <td>esco:HumanActor</td>\n",
       "      <td>0.6</td>\n",
       "      <td>less skilled, inexperienced operators</td>\n",
       "      <td>{'qname': 'esco:Skill', 'kind': 'class'}</td>\n",
       "      <td>esco:Skill</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Human actor</td>\n",
       "      <td>The addition of the human actor inside DTs pos...</td>\n",
       "      <td>class</td>\n",
       "      <td>http://www.w3.org/ns/activitystreams#Actor</td>\n",
       "      <td>0.8</td>\n",
       "      <td>human actor</td>\n",
       "      <td>{'qname': ':Actor', 'kind': 'class'}</td>\n",
       "      <td>:Actor</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>working skills</td>\n",
       "      <td>advanced automatic equipment presents a differ...</td>\n",
       "      <td>class</td>\n",
       "      <td>http://purl.org/ontology/mo/WorkingSkill</td>\n",
       "      <td>0.8</td>\n",
       "      <td>working skills</td>\n",
       "      <td>{'qname': 'esco:Skill', 'kind': 'class'}</td>\n",
       "      <td>esco:Skill</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>IMA E-CO Flex machine’s current status</td>\n",
       "      <td>1. AI4WORK shall provide a troubleshooting too...</td>\n",
       "      <td>instance</td>\n",
       "      <td>esco:Resource</td>\n",
       "      <td>0.8</td>\n",
       "      <td>ima e-co flex machine’s current status</td>\n",
       "      <td>{'qname': ':Status', 'kind': 'class'}</td>\n",
       "      <td>:Status</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>digital twin</td>\n",
       "      <td>3. The digital twin should model the functioni...</td>\n",
       "      <td>class</td>\n",
       "      <td>DigitalTwin</td>\n",
       "      <td>1.0</td>\n",
       "      <td>digital twin</td>\n",
       "      <td>{'qname': ':DigitalTwin', 'kind': 'class'}</td>\n",
       "      <td>:DigitalTwin</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>AI/robot</td>\n",
       "      <td>The SWS Management component aims to facilitat...</td>\n",
       "      <td>NonHumanActor</td>\n",
       "      <td>:NonHumanActor</td>\n",
       "      <td>0.9</td>\n",
       "      <td>ai/robot</td>\n",
       "      <td>{'qname': ':NonHumanActor', 'kind': 'class'}</td>\n",
       "      <td>:NonHumanActor</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>human</td>\n",
       "      <td>Depending on the respective work situation, th...</td>\n",
       "      <td>HumanActor</td>\n",
       "      <td>:HumanActor</td>\n",
       "      <td>0.9</td>\n",
       "      <td>human</td>\n",
       "      <td>{'qname': ':HumanActor', 'kind': 'class'}</td>\n",
       "      <td>:HumanActor</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>current level of uncertainty of the AI/robot</td>\n",
       "      <td>Depending on the respective work situation, th...</td>\n",
       "      <td>data_property</td>\n",
       "      <td>:hasConfidence</td>\n",
       "      <td>0.8</td>\n",
       "      <td>current level of uncertainty of the ai/robot</td>\n",
       "      <td>{'qname': ':hasConfidence', 'kind': 'data_prop...</td>\n",
       "      <td>:hasConfidence</td>\n",
       "      <td>data_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>the current work situation</td>\n",
       "      <td>Depending on the respective work situation, th...</td>\n",
       "      <td>Context</td>\n",
       "      <td>:Context</td>\n",
       "      <td>0.8</td>\n",
       "      <td>the current work situation</td>\n",
       "      <td>{'qname': ':Context', 'kind': 'class'}</td>\n",
       "      <td>:Context</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            span  \\\n",
       "1      Operational Equipment Effectiveness (OEE)   \n",
       "7          less skilled, inexperienced operators   \n",
       "8                                    Human actor   \n",
       "23                                working skills   \n",
       "34        IMA E-CO Flex machine’s current status   \n",
       "41                                  digital twin   \n",
       "44                                      AI/robot   \n",
       "45                                         human   \n",
       "46  current level of uncertainty of the AI/robot   \n",
       "47                    the current work situation   \n",
       "\n",
       "                                             sentence   concept_kind  \\\n",
       "1   Packaging material production lines are genera...          class   \n",
       "7   Given the repetitive and demanding nature of m...       instance   \n",
       "8   The addition of the human actor inside DTs pos...          class   \n",
       "23  advanced automatic equipment presents a differ...          class   \n",
       "34  1. AI4WORK shall provide a troubleshooting too...       instance   \n",
       "41  3. The digital twin should model the functioni...          class   \n",
       "44  The SWS Management component aims to facilitat...  NonHumanActor   \n",
       "45  Depending on the respective work situation, th...     HumanActor   \n",
       "46  Depending on the respective work situation, th...  data_property   \n",
       "47  Depending on the respective work situation, th...        Context   \n",
       "\n",
       "                                 ontology_term  confidence  \\\n",
       "1                             esco:Performance         0.8   \n",
       "7                              esco:HumanActor         0.6   \n",
       "8   http://www.w3.org/ns/activitystreams#Actor         0.8   \n",
       "23    http://purl.org/ontology/mo/WorkingSkill         0.8   \n",
       "34                               esco:Resource         0.8   \n",
       "41                                 DigitalTwin         1.0   \n",
       "44                              :NonHumanActor         0.9   \n",
       "45                                 :HumanActor         0.9   \n",
       "46                              :hasConfidence         0.8   \n",
       "47                                    :Context         0.8   \n",
       "\n",
       "                                       span_norm  \\\n",
       "1      operational equipment effectiveness (oee)   \n",
       "7          less skilled, inexperienced operators   \n",
       "8                                    human actor   \n",
       "23                                working skills   \n",
       "34        ima e-co flex machine’s current status   \n",
       "41                                  digital twin   \n",
       "44                                      ai/robot   \n",
       "45                                         human   \n",
       "46  current level of uncertainty of the ai/robot   \n",
       "47                    the current work situation   \n",
       "\n",
       "                                           match_meta   matched_qname  \\\n",
       "1          {'qname': ':Operational', 'kind': 'class'}    :Operational   \n",
       "7            {'qname': 'esco:Skill', 'kind': 'class'}      esco:Skill   \n",
       "8                {'qname': ':Actor', 'kind': 'class'}          :Actor   \n",
       "23           {'qname': 'esco:Skill', 'kind': 'class'}      esco:Skill   \n",
       "34              {'qname': ':Status', 'kind': 'class'}         :Status   \n",
       "41         {'qname': ':DigitalTwin', 'kind': 'class'}    :DigitalTwin   \n",
       "44       {'qname': ':NonHumanActor', 'kind': 'class'}  :NonHumanActor   \n",
       "45          {'qname': ':HumanActor', 'kind': 'class'}     :HumanActor   \n",
       "46  {'qname': ':hasConfidence', 'kind': 'data_prop...  :hasConfidence   \n",
       "47             {'qname': ':Context', 'kind': 'class'}        :Context   \n",
       "\n",
       "     matched_kind  \n",
       "1           class  \n",
       "7           class  \n",
       "8           class  \n",
       "23          class  \n",
       "34          class  \n",
       "41          class  \n",
       "44          class  \n",
       "45          class  \n",
       "46  data_property  \n",
       "47          class  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_df = pd.DataFrame(all_mapped)\n",
    "if not mapped_df.empty:\n",
    "    mapped_df[\"span_norm\"] = mapped_df[\"span\"].astype(str).str.lower().str.strip()\n",
    "    known_qnames = set(onto_df[\"qname\"].str.lower())\n",
    "    known_iris = set(onto_df[\"iri\"].str.lower())\n",
    "\n",
    "    def pick_meta(term: str, span_text: str):\n",
    "        \"\"\"Map to ontology via exact → embedding → fuzzy.\"\"\"\n",
    "        # 1) exact qname/IRI\n",
    "        if term:\n",
    "            k = str(term).lower().strip()\n",
    "            if k in known_qnames or k in known_iris:\n",
    "                return {\"qname\": term, \"kind\": key_to_meta.get(k, {}).get(\"kind\", \"\")}\n",
    "\n",
    "        # 2) embeddings (prefer span text)\n",
    "        if USE_EMBEDDINGS and E_onto is not None:\n",
    "            qtxt = (span_text or term or \"\").strip()\n",
    "            if qtxt:\n",
    "                qv = embed_texts_ollama([qtxt], OLLAMA_EMBED_MODEL, batch=1)\n",
    "                qv = l2_normalize(qv)[0]\n",
    "                sims = E_onto @ qv\n",
    "                top_idx = sims.argsort()[-EMBED_TOPK:][::-1]\n",
    "                best_idx = int(top_idx[0])\n",
    "                best_key = vocab[best_idx]\n",
    "                best_sim = float(sims[best_idx])\n",
    "\n",
    "                # Blend with fuzzy for stability\n",
    "                from rapidfuzz import process, fuzz\n",
    "                fuzzy_match = process.extractOne(qtxt.lower(), list(key_to_meta.keys()), scorer=fuzz.WRatio)\n",
    "                fuzzy_score = (fuzzy_match[1] / 100.0) if fuzzy_match else 0.0\n",
    "\n",
    "                blended = COMBO_ALPHA * best_sim + (1.0 - COMBO_ALPHA) * fuzzy_score\n",
    "                if (best_sim >= EMBED_MIN) or (blended >= COMBO_MIN):\n",
    "                    meta = key_to_meta.get(best_key)\n",
    "                    if meta:\n",
    "                        return {\"qname\": meta[\"qname\"], \"kind\": meta[\"kind\"]}\n",
    "\n",
    "        # 3) fuzzy fallback\n",
    "        if key_to_meta:\n",
    "            q = (span_text or term or \"\").lower().strip()\n",
    "            cand = process.extractOne(q, list(key_to_meta.keys()), scorer=fuzz.WRatio)\n",
    "            if cand and cand[1] >= 90:\n",
    "                meta = key_to_meta[cand[0]]\n",
    "                return {\"qname\": meta[\"qname\"], \"kind\": meta[\"kind\"]}\n",
    "        return None\n",
    "\n",
    "    metas = [pick_meta(row.get(\"ontology_term\",\"\"), row.get(\"span\",\"\")) for _, row in mapped_df.iterrows()]\n",
    "    mapped_df[\"match_meta\"] = metas\n",
    "    mapped_df = mapped_df[mapped_df[\"match_meta\"].notnull()]\n",
    "    mapped_df[\"matched_qname\"] = mapped_df[\"match_meta\"].apply(lambda m: m[\"qname\"] if m else \"\")\n",
    "    mapped_df[\"matched_kind\"] = mapped_df[\"match_meta\"].apply(lambda m: m[\"kind\"] if m else \"\")\n",
    "else:\n",
    "    mapped_df = pd.DataFrame(columns=[\n",
    "        \"span\",\"sentence\",\"concept_kind\",\"ontology_term\",\"confidence\",\"span_norm\",\"match_meta\",\"matched_qname\",\"matched_kind\"\n",
    "    ])\n",
    "\n",
    "new_df = pd.DataFrame(all_new).fillna(\"\")\n",
    "\n",
    "print(\"Aligned mapped mentions:\", len(mapped_df))\n",
    "print(\"New concept candidates:\", len(new_df))\n",
    "mapped_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e25b115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ./outputs\\dso_outline.md\n",
      "Also wrote: ./outputs\\mapped_mentions.csv and ./outputs\\new_concepts.csv\n"
     ]
    }
   ],
   "source": [
    "used_classes = sorted(set(mapped_df.loc[mapped_df[\"matched_kind\"]==\"class\", \"matched_qname\"].tolist()))\n",
    "used_obj_props = sorted(set(mapped_df.loc[mapped_df[\"matched_kind\"]==\"object_property\", \"matched_qname\"].tolist()))\n",
    "used_data_props = sorted(set(mapped_df.loc[mapped_df[\"matched_kind\"]==\"data_property\", \"matched_qname\"].tolist()))\n",
    "\n",
    "def group_candidates(df: pd.DataFrame):\n",
    "    groups = {}\n",
    "    if df.empty:\n",
    "        return groups\n",
    "    for tg in [\"class\",\"object_property\",\"data_property\",\"\"]:\n",
    "        sub = df[df[\"type_guess\"].str.lower()==tg] if tg else df[df[\"type_guess\"]==\"\"]\n",
    "        if not sub.empty:\n",
    "            items = []\n",
    "            for _, r in sub.iterrows():\n",
    "                items.append({\n",
    "                    \"term\": r.get(\"term\",\"\"),\n",
    "                    \"definition\": r.get(\"definition\",\"\"),\n",
    "                    \"example_sentence\": r.get(\"example_sentence\",\"\"),\n",
    "                    \"rationale\": r.get(\"rationale\",\"\"),\n",
    "                    \"confidence\": r.get(\"confidence\",\"\")\n",
    "                })\n",
    "            groups[tg if tg else \"unspecified\"] = items\n",
    "    return groups\n",
    "\n",
    "groups = group_candidates(new_df)\n",
    "\n",
    "dso_lines = []\n",
    "dso_lines.append(\"# Domain-Specific Ontology (Text Outline)\\n\")\n",
    "dso_lines.append(\"## Reused Classes\")\n",
    "dso_lines += [f\"- {c}\" for c in used_classes] if used_classes else [\"- (none)\"]\n",
    "\n",
    "dso_lines.append(\"\\n## Reused Object Properties\")\n",
    "dso_lines += [f\"- {p}\" for p in used_obj_props] if used_obj_props else [\"- (none)\"]\n",
    "\n",
    "dso_lines.append(\"\\n## Reused Data Properties\")\n",
    "dso_lines += [f\"- {p}\" for p in used_data_props] if used_data_props else [\"- (none)\"]\n",
    "\n",
    "dso_lines.append(\"\\n## New Class Candidates\")\n",
    "for item in groups.get(\"class\", []):\n",
    "    dso_lines.append(f\"- **{item['term']}** — {item['definition']}  (e.g., \\\"{item['example_sentence']}\\\")\")\n",
    "\n",
    "dso_lines.append(\"\\n## New Object Property Candidates\")\n",
    "for item in groups.get(\"object_property\", []):\n",
    "    dso_lines.append(f\"- **{item['term']}** — {item['definition']}  (e.g., \\\"{item['example_sentence']}\\\")\")\n",
    "\n",
    "dso_lines.append(\"\\n## New Data Property Candidates\")\n",
    "for item in groups.get(\"data_property\", []):\n",
    "    dso_lines.append(f\"- **{item['term']}** — {item['definition']}  (e.g., \\\"{item['example_sentence']}\\\")\")\n",
    "\n",
    "dso_lines.append(\"\\n## Unspecified-Type Candidates\")\n",
    "for item in groups.get(\"unspecified\", []):\n",
    "    dso_lines.append(f\"- **{item['term']}** — {item['definition']}  (e.g., \\\"{item['example_sentence']}\\\")\")\n",
    "\n",
    "outline_path = os.path.join(OUT_DIR, \"dso_outline.md\")\n",
    "with open(outline_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(dso_lines))\n",
    "\n",
    "mapped_csv = os.path.join(OUT_DIR, \"mapped_mentions.csv\")\n",
    "new_csv = os.path.join(OUT_DIR, \"new_concepts.csv\")\n",
    "mapped_df.to_csv(mapped_csv, index=False, encoding=\"utf-8\")\n",
    "new_df.to_csv(new_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Wrote:\", outline_path)\n",
    "print(\"Also wrote:\", mapped_csv, \"and\", new_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
