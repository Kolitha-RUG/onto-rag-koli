# AI4Work-D1.1 (2024) - Pilots Requirements Analysis & Update of SotA

## 2. Description of Pilots

### 2.2 Manufacturing Pilot

Packaging material production lines are generally highly automated yet susceptible to multiple minor disruptions, which significantly impact Operational Equipment Effectiveness (OEE). This susceptibility is largely due to the complexity and sensitivity of various process stages, combined with stringent production speed and quality requirements. In the context of food and liquid food packaging, these operations are critical for safety, as any deviation in product quality can directly jeopardize consumer health and safety. Consequently, this leads to inconsistent production reliability across different shifts, characterized by more frequent production halts and extended repair times, particularly during night shifts which lack experienced operators (despite the need for continuous operation). The human roles involved in these automated processes fall into three categories: (1) Line Supervisor, who ensures production volume and quality; (2) Machine Attendant, who handles tasks that are not yet automated, such as loading and unloading; and (3) Machine Maintainer, who resolves specific production issues and performs routine maintenance. These roles are theoretical and can overlap in a single individual working on the line, leading to high variability in the tasks required by operators and a wide range of operators' skills and experience levels. Given the repetitive and demanding nature of machine tending, the manufacturers face a high turnover rate among employees, creating a paradox where a few highly skilled operators, capable of addressing various machine and product failures, coexist with a larger group of less skilled, inexperienced operators. The manufacturing pilot aims to address this issue with an AI-powered digital production assistant designed to bridge the knowledge gaps among operators and between operators and machines. The envisioned digital assistant will:
(a) Capture knowledge from experienced workers by documenting maintenance and repair operations. This involves a crucial challenge: determining how to effectively and legally monitor operator actions. Possible methods include recording via cameras, using sensors or wearables, or having operators orally or in writing document their tasks.
(b) Link specific maintenance and repair operations to different types of failures, assessing and ranking their effectiveness based on metrics such as mean time to repair and possibly the time until a similar or related failure occurs again.
(c) Provide targeted instructions to operators, tailored to their varying levels of experience to maximize productivity on the line.
(d) Collect feedback from users and adapt based on this input.
(e) Assist operators in planning their daily or shift activities, optimizing time spent on various tasks and actions.



## 3. Update of State of the Art

### 3.4 Human representation in DT
With the emergence of the Industry 5.0 paradigm, the introduction of the human element into DTs became a necessity, with a pressing need to address interaction among the physical, digital, and human world [40]. The addition of the human actor inside DTs poses new challenges. Human behaviour involves both objective and subjective elements and, therefore, modelling requires much more effort than needed for industrial systems and machines [41]. Adding to the complexity of such modelling, concerns regarding data protection and privacy create constraints for the potential technological solutions and their admissibility within ethical and legal boundaries. 
In the manufacturing sector, based on 2021 data, over 70% of tasks continue to be performed manually, placing a significant portion of value creation on human labour. Additionally, it is projected that by 2025, the average time humans and machines spend working will be comparable to the levels seen in 2021 [42]. 
Two main aspects emerge when considering human representation in DT:
The first is the creation of a digital twin of the human being itself which, similarly to the creation of a DT of an industrial system, aims to create a digital representation of the person, including physical parameters and possibly including a data connection monitoring the physical state of the individual. This brings into consideration questions about what is an appropriate level of abstraction for a human representation within an industrial DT. 
This type of digital twin has been developed mainly in the biomedical sector and aims to create models able to predict the arising of health conditions or improve the physical and psychological conditions of the person. Such a type of system can assume the form of IoT-based context-aware healthcare monitoring systems, as in [43] and [44], or systems which aim to monitor the fatigue of workers using wearable devices as in [45] and [46]. However, privacy concerns may limit the level of personalisation that is permissible, and cross-subject generalisation of such monitoring systems remains a challenge [47]. 
The second aspect is the addition of the human factor into the DTs of production systems. The role of the operator changed in industry 4.0, and the operator 4.0 has been defined as a smart and skilled operator who does not only perform cooperative work with robots but also works aided by machines and if needed by means of Human-Cyber-Physical-Systems, advanced human-machine interaction technologies and adaptive automation towards achieving human-automation symbiosis work systems. [48].
Adding the operator representation into the context can fulfil different purposes and allow the DT to become a more complete tool becoming not only the digital representation of machines and equipment but including the workers and the human aspects in the loop.
HDT have been applied in applications to improve production rates as in [49] and [50], where HDTs are proposed to collect and communicate workers’ skills, preferences, personality and to enable humans to take part to a decentralized computational decision-making process which leads to an improved scheduling. In this type of application is important to consider that there are several taxonomies and methods that exist to assess workers skills, like ESCO and O*NET, but humans’ learning and skill representation is still a challenge [51].
Another scope for the application of HDT is the improvement of human-robot collaboration in an industrial environment. With the growing adoption of collaborative robots and the increasing presence of industrial environments with a mixed presence of humans and robots, the design and management of such environments can benefit from the use of HDTs, for example including the human presence in real-time control loops as in [52] or the acquisition of human operator movement used in [53] to optimize real time robot trajectories to avoid collision. The HDT can also be used to increase the support of automation or AI to the human operator as in [54], where the support given from a collaborative robot to the human operator was adjusted on the basis of the fatigue level of the worker monitored using real-time sensor data.
All the aforementioned uses of HDTs require different aspects of the human to be represented, but the technological structure of HDTs is similar to the one of traditional DTs [42], with a sensor layer which acquires data from the field, a middleware to interface the DT with the sensor layers and a core module containing the data persistence module, the governing module and the behavioural and decision module.
The main additions are in the data persistence module where an adequate data model needs to be integrated for the representation of the workers and their interactions with other assets of the system and in the behavioural and decision module, where the scientific challenge resides, to include human factors, with the uncertainty and variability of the human behaviour.

### 3.6 Sliding Work Sharing
AI4Work investigates practical methods and tools for optimal sharing of work between humans and AI/robots. In this context, "sliding work sharing" is defined as an approach where the balance between human and machine activities varies during the operation, depending on the situational context, machine-based confidence levels and human interactions [80]. This term and definition were inspired by the similar definition of "sliding decision making", as presented in [88]. 
The term "sliding work sharing" (SWS) was introduced by the AI4Work project proposal [80], therefore no prior scientific literature can be found about SWS. However, the related concept of "sliding autonomy" could be identified, which is considered highly relevant as basis for AI4Work's research on SWS. Sliding autonomy, also called "adjustable autonomy", was researched extensively in the domains of robotics and (multi-)agent systems [82]. Instead of having fixed modes like "robot working autonomously" or "human teleoperating the robot", the aim is to vary the level of autonomy during operation, depending on the respective situation. The motivation for this approach is that, on the one hand, highly autonomous robots can help reducing the mental load of human operators [83] and they can also be used in hostile environments, where humans may not be able to work [81,84], but on the other hand, it is hardly possible (or very time-consuming) to program robots in a way that they can react to all potential challenges that may happen in dynamic real-world environments [81,83,87]. Therefore, humans should at least monitor the robots' work and should be able to intervene in case of robots "being stuck" or in case of potential safety hazards. Another motivation for sliding autonomy may be to dynamically adjust the robots' autonomy level depending on the skill level of the human who is controlling the robot [86]. 
Different scenarios regarding sliding autonomy are discussed in the existing literature, involving either single robots (e.g. [86]) or teams of robots, either being supervised by humans (e.g. [81,83]) or working with humans in peer-to-peer mode (i.e. humans are working together with robots on the same task or can even get tasks assigned by robots, e.g. [82,87]), solving different experimental tasks. Aiming to simplify the description, the example of “one human and one robot working together” is used in the following, however the general principles apply also to multi-robot/multi-human teams. 
A variety of potential levels of autonomy may be differentiated, e.g. considering the ten levels of automation suggested in [85]. However, when considering sliding autonomy of robots, the three main levels that are most relevant can be summarized as follows:
    • Fully autonomous: the robot is working autonomously, without any human involvement.
    • Intermediate: the robot is in general working autonomously, but a human can temporarily provide support or take over control.
    • Manual operation: the human operates the robot, having full control over it.
A temporal human involvement, as indicated in the intermediate autonomy level above, is typically caused by an unexpected issue that either prevents the robot from continuing its work or that decreases the performance of its work. Such unplanned human involvements may be "triggered" by either human or robot, so two basic scenarios can be differentiated:  
    • The robot itself detects an issue and informs the human that it requires support. This may happen for different reasons, e.g.
        ◦ the robot is blocked/trapped or is lost in the environment [82],
        ◦ considering a given task, the robot has a low level of "self-confidence" regarding its ability to succeed [84],
        ◦ a timeout occurred, i.e. the robot did not succeed after a specific time or number of tries [81,87],
        ◦ the robot actively assigns a task to a human, which is relevant in peer-to-peer human-robot teams [82].
    • The human pro-actively intervenes because they observe one of the following: 
        ◦ the robot's work seems inefficient and thus efficiency improvements are possible through human intervention [81],
        ◦ the robot is behaving incorrectly [87],
        ◦ the robot may become a safety hazard (e.g. due to potential collisions with surrounding objects, other robots, or even humans) [81,84,86],
        ◦ there may be a risk of losing the robot, which is especially relevant when working in hostile environments where humans can only teleoperate the robot [84].
If only the first scenario is involved, i.e. the human only reacts to the robot asking for help, this is called "system-initiative sliding autonomy", while if also the human can pro-actively intervene, this is called "mixed-initiative sliding autonomy" [83,87]. 
Besides being caused by unexpected issues, human involvement can also be pre-planned, especially if it is known in advance that one specific task - out of a well-known set of tasks to be executed - is hard to automate. After the human has done that particular task, the robot can continue to work on its own [81,84].
Different studies about sliding autonomy did experiments (either physical or simulated) to compare the performance (including required time, error rate, workload for human operator) of sliding autonomy compared to robots being either fully autonomous or under full control of humans, e.g. [81,82,83,86,87].  Although these experiments usually comprise only a limited number of repetitions of a very specific task, some general patterns can be observed. Sliding autonomy typically leads to a higher success rate compared to fully autonomous robots (as humans can help in case robots fail), as well as to faster task solving compared to fully manual operation (because it does not require the human to control every step in detail, which can be tedious and is especially difficult when controlling a multi-robot team). The downside may be, on the one hand, slower task solving compared to fully autonomous robots, and on the other hand, higher failure rates compared to the robot being under full control of a human. 
Sliding autonomy can thus be considered a compromise/trade-off approach, aiming to find the ideal balance between human and machine activities depending on the respective situation, as it is the goal of SWS. One strategy may be to increase the autonomy level step by step, thus increasing the efficiency and reducing the human workload, but only until the performance of the robot starts to deteriorate [84]. Another aspect to be considered is that the ideal degree of autonomy may be dependent on the experience/skills of the human who is controlling and supporting the robot [83].
In most of the AI4Work pilots, the work sharing will be between human and AI, while only few use cases will deal with work sharing between human and robot. Therefore, when considering the state of the art regarding sliding autonomy of robots in the upcoming development of SWS mechanisms, a basic difference between robots and AI must be considered: a human can rather easily see that a physical robot requires help or could become a safety hazard, but this may not be as easy or even impossible for a human working together with and observing an AI. Sometimes the AI may provide answers of low accuracy or trustworthiness in case the basis to learn from was not very solid (e.g. learning data sets that were either too small or did not cover all situations that occur in the real work environment). Therefore, the above-mentioned topics of safe AI monitoring (section 3.5.2) as well as explainable/transparent AI will be important to consider in relation to SWS.

## 4. Pilot Specific SotA review

### 4.2 Pilot 2 - Manufacturing
            4.2.1 Defining the Problem
The rapid expansion of the e-commerce sector encouraged mid-to large-sized companies to invest in automation technologies and human resources to respond to a constantly increasing trend in terms of procurement, packing and delivery operations. On the one hand, enterprises involved in the e-commerce business are actively seeking industrial technologies to efficiently enhance the throughput of their packing lines, thereby introducing an increased degree of automation; on the other hand, they strive to recruit a broad spectrum of personnel, ranging from machine attendants and maintainers to line supervisors and production managers. Due to the scarcity of specialized workforce, and to the strain and tediousness of several machine-attending tasks, such recruitment becomes a difficult task. This leads to high turnover rates among employees, poor experience abilities and unclear distinctions between personnel roles. Highly automated technologies are expected to fill these gaps, introducing an increased level of un-manned operations. Yet, advanced automatic equipment presents a different set of challenges with respect to traditional manufacturing, addressed by a specific range of working skills. While automated packing stations assist workers in common duties simplifying their task, machine operators and maintainers are also expected to analyse unforeseen situations and provide solutions up to a certain extent of complexity. At the same time, line or production managers, accountable for the efficiency of the area under their responsibility, face new challenges in terms of planning plant activities and matching people’s availability and technical skills with production issues to be solved.
In this scenario, artificial intelligence may play a crucial role in assisting inexperienced operators and guiding novice managers in their choices. Within the framework of AI4Work, and possibly beyond the duration of the project, the developed technologies will be utilized to address the issues that arise in the packaging pilot use cases. AI4Work solutions will address three common situations in a production environment: troubleshooting an unexpected stoppage or failure; guiding inexperienced operators in performing standard tasks; and planning plant activities to maximize productivity. These tasks may appear trivial at first, or in line with the state of the art, at most. The vision of AI4Work, however, includes broader methods intended to optimally balance human intervention and resources with AI (or robotic) support. Indeed, these capabilities perfectly match the needs described in the presented scenario. Human work is generally a viable resource for plant managers. However, the availability and skill level of such a workforce are variable, unpredictable, and often insufficient for the required tasks. AI methods, in conjunction with digital twins and other data tools, serve as ideal companions for enhancing human collaboration, facilitating human-to-machine interactions, and recommending the most appropriate strategies to schedule production tasks within a plant, considering individuals’ aptitudes and availability. 
            4.2.2 Data and Resources (Hardware/Infrastructure/Sensors) Involved
The context of the described scenario involves an e-commerce packaging/boxing plant populated with several IMA E-CO Flex machines, machine attendants, machine maintainers, and a line manager. The E-CO Flex machine serves as the hardware asset that the AI4Work technologies will interact with. The purpose of the machine is to assist the attendant in packing operations such as box selection, content inspection/checking, box downsizing, folding, taping, labelling, etc. The insertion of items into the box is still performed manually and considered out of the scope of the pilot.
The machine is made of mechanical, electrical and software components. A central programmable device, namely the machine PLC (programmable logic controller), is responsible for coordinating electric actuators that, in turn, control the movement of mechanical surfaces and tools onboard. Most electromechanical actuators provide physical feedback to the machine controller, usually via electrical currents or optical signals. The former feedback type is integrated in motion drives, whereas the latter is provided by dedicated optical sensors, which include detectors, laser meters, and cameras. Pneumatic gripping systems, such as vacuum suckers, are also used, and include pressure transducers to monitor the vacuum level.
The human machine interface (HMI) is the principal system for human interaction with the machine. The HMI hardware, an industrial PC with an integrated display, runs the HMI software, a program that exchanges data with the PLC. At the same time, the HMI presents a graphical interface on the touchscreen for human interaction in terms of machine start/stop, configuration, troubleshooting, etc.
PLC and HMI retain all the data available on the E-CO Flex machine. According to a simplified view of hardware/software components, the distinction between PLC data and HMI data is straightforward: although numerous data originate from the PLC (e.g., physical signals, alarms, machine states), most information is available on both platforms, with the HMI keeping a historical database of events and performance. The HMI also collects external inputs, such as user’s interactions and production orders, before transmitting corresponding commands to the PLC, if needed. Some very low-level information, such as real-time values of motor currents or pressure levels, may only be available on the PLC, resulting in a limited historical view.
            4.2.3 Existing Methodologies Applied
Most IMA packaging solutions rely on an industrial edge device, called A4Gate, to collect ma-chine data and share them with a proprietary cloud platform, namely the IMA Data Room. Information available on the Data Room is aggregated and made available to users on a web portal, the IMA Sentinel dashboard. First-order analytics are also presented, providing some insights online productivity and issues that may affect its performance, such as in [134]. In addition, some machine learning models are being trained on available data to identify, anticipate, and possibly avoid machine faults by recommending some actions to the operator [135]. The effectiveness of such methods, however, strongly depends on the amount and quality of data collected for the monitored machine model. The E-CO Flex makes no exception; thus, since E-CO Flex is a novel solution for IMA, the available information is not sufficient for this approach for now.
The discussed approaches do not contemplate human participation in the decision-making pro-cess, or their availability, experience, or capability when computing a response to the end-user. Nevertheless, including human interactions along the data processing loop would foster a more sustainable AI contribution. Explainable, trustworthy, and human-centred AI models are considered to be key elements for matching artificially computed solutions with human needs and workers’ experiences. As a result, well-established and widely accepted AI solutions would facilitate data sharing, which would further improve the balance between human and digital workloads.
            4.2.4 Measurable Results (KPIs) Identified in other relative studies
In 2021, the European Committee for Standardization released the European Standard EN 415-11, which standardized the overall equipment effectiveness (OEE) as a measure of efficiency for the manufacturing industry [136]. The OEE is determined by the product of three factors: quality, performance, and availability. Given a unit of time, the quality expresses the good units produced as a fraction of the total units started; thus, it accounts for rejection losses. The performance measures the running speed of the equipment with respect to the designed speed; thus, it accounts for speed losses or minor stoppages. The availability is the fraction of the scheduled time that the equipment is available to operate (also referred to as uptime); thus, it accounts for unplanned downtime due to breakdowns or disruptions.
The OEE expresses an overall indication on the manufacturing process efficiency over a time period and does not reflect the instant performance of a particular piece of equipment. Although the OEE is affected by the effectiveness of human actions adopted to operate the machine (i.e., an inexperienced operator may contribute to lower the OEE), a factor accounting for the working conditions (e.g., quality, sustainability, stress, tediousness, etc.) is still missing. 


## 7. List of Requirements and Use Cases
### 7.2 Pilot 2 - Manufacturing Sector
Use Case Identification and History
Use Case ID:	UC-MAN-1
Use Case Name:	Troubleshooting
Description:	A high turnover rate prevents operators from developing deep expertise in their duties, including troubleshooting. However, their accumulated experiences can form a substantial knowledge base for future employees. This knowledge can be expanded upon and shared among colleagues and successors, offering valuable guidance.
Introducing a Digital Assistant that stores and manages this expertise becomes crucial. When machine or production failures or inefficiencies occur, the assistant should promptly suggest the most suitable recovery/adjustment measures to implement. Additionally, AI support is essential in aiding maintenance operators to interpret failures/inefficiencies and formulate responses based on historical data from past records and machines status associated with these records.
Scenario:	An E-CO Flex machine encounters a fault beyond the expertise of the maintenance operator. The machine operator contacts maintenance assistance, opening a ticket that provides information about the current state of the machine and, eventually, other useful information like production data and sensor readings. Based on this information and on historical data, AI interprets the failure and gives troubleshooting support. Historical data includes closed tickets detailing machine states, previous faults, the state of the involved machine at that time, and the troubleshooting routines used to solve the fault.
Technologies:	    • Data collection and handling for AI/Robotics Services.
    • Human-Centred Digital Twins of processes and employees.
    • Context Awareness.
    • Sliding Work Sharing.
User/Actors:	Maintenance operator.
Machine operator.
Requirements and Prioritization:
Req. ID	Requirements	Priority
(shall, should, may)
1	Provide a troubleshooting tool able to suggest solutions based on current faults and historical data	Shall
2	Recognize previously unknown faults
 	Should
3	Use a digital twin to simulate the machine behaviour over time	May
Requirement Details:

    1. AI4WORK shall provide a troubleshooting tool that utilizes the IMA E-CO Flex machine’s current status and fault data, in conjunction with historical fault records and machine learning models, to accurately diagnose issues and recommend appropriate solutions.
    2. Given a set of data collected from the IMA E-CO Flex machines and a set of associated faults, the troubleshooting tool should be capable of recognizing and identifying new or different faults that are not already present in the database.
    3. The digital twin should model the functioning of the IMA E-CO Flex machine over time, including the instants prior to the fault, to make it possible to observe the state of the machine at the occurrence of the fault and associate a specific fault with a specific operating condition of the machine.

Use Case Identification and History
Use Case ID:	UC-MAN-2
Use Case Name:	Training
Description:	A thorough reading and complete understanding of operating instructions is always a time-consuming task that many people skip in favour of some hands-on learning. For complex machinery, however, the result may not be the same. An on-demand comprehensive step-by-step procedure based on the official product documentation would represent a solid alternative to both traditional and convenience methods.
Having a tool that provides an adequate level of initial training to operators starting to work on a specific machine or production line, tailored according to their level of expertise, becomes crucial to reducing learning curves, minimizing errors, and maximizing productivity from the outset.
Scenario:	Operators have to improve their expertise in operating machines and executing maintenance procedures. They ask for support from a virtual assistant, which exploits natural language models to process information from the operator's manual and create an interactive virtual trainer tailored to the given machine model. This trainer assists operators by guiding them through common procedures, including maintenance tasks.
Technologies:	    • Data collection and handling for AI/Robotics Services.
    • Context Awareness.
    • Sliding Work Sharing.
User/Actors:	Machine operator.
Maintenance operator.
Requirements and Prioritization:
Req. ID	Requirements	Priority
(shall, should, may)
4	Provide a tool that supports machine operators for training and expertise development in machine usage	Shall
5	Accept questions in natural language	Shall
6	Provide answers in natural language	Should
7	Answers depend on the operator's expertise/skills	May
8	Interpret machine conditions that require human intervention	May
Requirement Details:
    4. AI4WORK shall provide a tool, in the form of virtual assistant, designed to support machine operators by facilitating training activities and fostering the development of expertise in the operation and maintenance of the E-CO Flex machine.
    5. The virtual assistant shall be capable of accepting and processing questions posed in natural language, enabling users with different levels of technical expertise to interact with the system and facilitating easy access to maintenance information.
    6. The virtual assistant should provide answers in natural language, allowing users to receive clear and understandable responses in a conversational style without requiring them to interpret technical or complex outputs.
    7. The virtual assistant's responses may vary in quality and depth based on the operator's level of expertise and skills.
    8. The virtual assistant may also autonomously discern machine conditions (such as active alarms) that would require human intervention in order to spontaneously support operators.

Use Case Identification and History
Use Case ID:	UC-MAN-3
Use Case Name:	Activity Scheduling
Description:	Automatic machines are constantly supplied with packaging materials, including tapes, labels, and boxes of various sizes. Despite operating at full capacity, their throughput varies over time due to factors such as the types of orders and goods being packed, operator expertise, and schedule efficiency. Reactive operations are often triggered when production halts, leading to efficiency losses. Automating the feeding of these machines and scheduling the overall production line, encompassing routine machine maintenance, loading/unloading operations, and visual quality controls, based on factors such as order urgency, raw material availability, machinery status, and personnel availability, becomes crucial. This approach maximizes productivity, minimizes downtime, and enhances overall operational efficiency significantly.
Scenario:	The scenario involves a set of input orders, each specifying products to be produced, a destination, and urgency levels. Additionally, there are human machine operators available, each with specific expertise, a variety of machines, and a stock of raw materials such as tapes, labels, and boxes in various sizes. The AI4WORK solution is designed to intelligently schedule the production line under these conditions, ensuring a balanced workload between human operators and machines. The line manager will utilize the output plan to assign activities accordingly, while also having the capability to introduce new constraints related to resource availability and workload balancing. The system will dynamically adjust the plan in real-time based on this feedback, optimizing production efficiency and responsiveness to changing operational needs.
Technologies:	    • Data collection and handling for AI/Robotics Services.
    •  Human-Centred Digital Twins of processes and employees.
    • Context Awareness.
    • Human-aware Task Planning for human-machine interaction.
    • Sliding Work Sharing.
User/Actors:	Machine operator. 
Line manager.
Requirements and Prioritization:
Req. ID	Requirements	Priority
(shall, should, may)
9	Provide a tool for the automatic generation of line plans considering available human and machine resources	Shall
10	Consider optimization metrics	Shall
11	Adapt the output plans to new requests of workload balancing between the human and the machine	Shall
12	Consider inputted constraints on available resources (e.g., materials)	Shall
13	Use a digital twin to simulate production line scenarios, including both the human operators and involved machines	Shall
Requirement Details:
    9. Given a set of input orders, each one characterized by a certain urgency and specific products to be delivered, AI4Work will develop a tool to automate the generation of production line plans. This tool will consider factors such as the availability of human operators with varying skill levels, the availability of machines, and the one of necessary raw materials for production.
    10. The generated plan shall consider optimization metrics with respect to selected resources like time or machine and materials utilization.
    11. The system shall dynamically adapt the output plans in response to new requests of workload balancing between human operators and machine resources.
    12. The output plans shall consider constraints on available resources, such as the quantity of raw materials that are currently accessible for production, ensuring efficient and practical scheduling of the production line operations.
    13. The digital twin shall simulate production line scenarios, modelling the operation of the machines and assessing the capabilities of human operators involved in the processes.



# AI4Work-D1.2 (2024) - AI4Work Concept
## 3. Technological Concept

### 3.8 Sliding Work Sharing 
3.8.1 Component Overview
The SWS Management component aims to facilitate dynamic sharing of work between humans and AI/robots. Depending on the respective work situation, the SWS Management shall decide about:
    • the required degree of involvement of the human, considering the current level of uncertainty of the AI/robot,
    • the (degree of) support to be provided to the human, depending on the human's experience and the current work situation.
The level of human/machine involvement is not considered to be fixed in advance but may vary at “runtime”, reacting to unplanned or unexpected events. This approach can be depicted as a “slider” between human and machine activity, which is adjusted dynamically, depending on the respective work situation. Three examples of the most typical situations are visualised as such a “slider” in the following figures:
    • In case of high uncertainty of the AI in an unusual situation, the decision may fall to the human (see Figure 14).
    • In case of low uncertainty of the AI in a common situation, the decision/action may be suggested by the AI and confirmed by the human (see Figure 15).
    • In case of very low uncertainty of the AI in a very common situation, the decision may be automatically made by the AI, with human intervention possible but optional (see Figure 16).

Figure 14: Possible SWS situation (i)1 

Figure 15: Possible SWS situation (ii)

Figure 16: Possible SWS situation (iii)
It should be pointed out, however, that the “slider” is not limited to the aforementioned situations but may move on the whole scale between “full human control” and “fully autonomous AI/robot”. 
As some application scenarios may include several AI modules (e.g. one to support work planning in advance and one to support dynamic adaptations at plan execution time), the SWS Management may also support “dynamic sliding between two AI modules”. However, only the case of SWS between human and AI is considered in the following, aiming to keep this concept description simple.
3.8.2 Key Challenges
The key challenges for the Sliding Work Sharing component in AI4Work are:
    • Make the decision about the degree of human involvement vs. the autonomy of the AI/robot 
AI4Work approach: the SWS Management will take several inputs from other components into account as basis for this decision, the most important being:
        ◦ The uncertainty estimation regarding the AI/robot 
        ◦ The context information about the current work situation
        ◦ The experience/skills of the human
    • Make the decision about the need for re-planning/re-scheduling in case of an unexpected situation at “plan execution time” 
AI4Work approach: the SWS Management will take several inputs from other components into account as basis for this decision, the most important being 
        ◦ The results of the plan’s validation
        ◦ The context information about the current work situation
        ◦ The experience/skills and availability of human operator(s)
        ◦ The uncertainty estimation regarding the AI/robot
    • How to adapt to the different pilot use cases, which consider a large variety of human-AI/robot work sharing situations?
AI4Work approach: the SWS Management will not be one monolithic component with a “one-size-fits-all” approach, but will rather provide generic reusable framework elements, which can be adapted and extended flexibly for each concrete pilot use case.
3.8.3 Concept
SWS is defined as a work-sharing approach where the balance between human and machine (AI or robot) activities varies during the operation, depending on the situational context, machine-based confidence levels and human interactions [27] The SWS Management component aims to take these different inputs into account when deciding about the appropriate degree of human involvement – and/or the degree of support to the human – depending on the respective work situation.
A simplified conceptual workflow for SWS, indicating the role of the SWS Management component and its inputs from other AI4Work components, is depicted in Figure 17. 

Figure 17: Sliding Work Sharing workflow2
The workflow may be initiated by a human, a robot, some AI4Work technology (e.g. a Digital Twin that monitors the processes, the Context Awareness component detecting a situation change, or some AI service) or an external system. The “request for action” indicated on the left side of Figure 17 should thus be considered a “generic placeholder” for “a work situation where some action/decision is required”. To enable the decision about the human/AI/robot involvement in the respective situation, several AI4Work technologies/components need to provide information:
    • The Context Awareness component extracts the contextual information about the current work situation
    • The AI/robot suggests a decision/action based on the context information
    • The Runtime Monitoring for trustworthy AI takes the suggested decision/action as well as the context information into account, estimating the confidence/uncertainty with which the AI/robot is making a decision/action in the current situation
    • The Human-centred Digital Twin needs to provide information about the skills/experience of the human
Based on these inputs, the SWS Management component then “decides” or “recommends” to what extent the AI/robot may work autonomously and if the actions/decision should be requested from (or checked/confirmed by) the human operator. 
Depending on the specific application scenario, different basic degrees of automation may be possible. In some cases, the AI/robot may work autonomously as long as the uncertainty level of AI decisions is low. In other cases, the human operator may want to always stay in control, so that each decision/action suggested by the AI may have to be confirmed by the human.
The main focus of the SWS Management is not on “planning work sharing in advance”, but to react to events/situations at “plan execution time”. However, in case of major disruptions that invalidate the original work plan, the SWS Management component may also decide that a re-calculation of the work plan is required. In such a case it will trigger other AI4Work components to do a re-planning. 
An initial conceptual architecture of the SWS Management component is shown in Figure 18. The main elements are the following:
    • Sliding module: this module is responsible for the “dynamic sliding decision at runtime”. Its “rule storage” contains pilot/use-case-specific rules that must be defined in advance. At runtime the “decision engine” applies these rules to the current input data (which is received from other AI4Work components and pilot-specific AI components) in order to dynamically decide about the recommended degree of AI autonomy and/or support to the human in the current situation. 
    • Workflow Orchestrator: this module controls the flow of inputs from and outputs to other systems/components, thus facilitating the integration of the SWS Management into application scenarios with different workflows and connected systems. The use-case-specific workflow needs to be defined in advance and is kept in the “workflow definition storage”. At runtime, the “workflow executor” reads this workflow definition and orchestrates the work:
        ◦ It triggers other systems/components and requests input data.
        ◦ It asks the Sliding Module for a decision.
        ◦ It provides the outcome of the “sliding decision” to either of the following:
            ▪ An existing (legacy) system, which can adapt its behaviour based on the decision/recommendation of the SWS Management component.
            ▪ Some use-case-specific custom GUI (provided via the AI4Work Common Frontend), which can show the decision to an end-user. Depending on the expertise of the human end-user, this GUI may also present the identified context as well as the estimated uncertainty of the AI/robot to increase understandability/explainability.

Figure 18: Conceptual Architecture of the SWS Management component3
3.8.4 Innovations Targeted
    • Application of the concepts of “sliding autonomy” [28], [29], [30] and “sliding decision making” [31] to the domain of “work sharing between human and AI”. 
    • Generic component/framework for SWS Management that can be adapted/extended to be applied in various pilot use cases. 
3.8.5 Inputs/Outputs
3.8.5.1 Inputs
Inputs to the SWS Management will be:
    • Current context information about the work situation from the Context Awareness component
    • Uncertainty estimation from the Runtime Monitoring for Trustworthy AI
    • Data about the experience/skills of the human (from the Human-centric Digital Twin)
    • Feedback from the human that is interacting with the Common Frontend or existing system
    • Plans from the Human-aware Task Planner and Long-term Adaptation components
    • Suggested decisions/actions from pilot-specific AI modules
    • Trigger messages about events that require a “sliding decision” (such triggers may come from the Context Awareness component, from an external system or from the end user) 
3.8.5.2 Outputs 
Outputs will be: 
    • Trigger messages to other components, e.g. trigger Human-aware Task Planner or Long-term Adaptation components to recalculate their plan or validate a plan.
    • Requests to other systems/components to provide required input data.
    • Decision/suggestion regarding “degree of human involvement” or “degree of support to human” (so called sliding decision) to another system (e.g. legacy system that the human interacts with, or some pilot-specific GUI that presents it to the human operator).
    • Information about the current uncertainty of the AI to the GUI, so that the human gets transparent information about the current AI activity (allowing to double-check/intervene in case of medium-to-high uncertainty).
3.8.6 Component Requirements
    1. Shall: Decide about the degree of human involvement that is required in the respective work situation, depending on the uncertainty of the AI.
    2. Shall: Support customization to different pilot use cases via configurable “sliding rules”. 
    3. Shall: Take into account the user preferences and/or company guidelines regarding the AI autonomy level (e.g. some users may want to always double-check AI decisions, other users may trust the AI to work in automatic mode, some company may want to assure that there is always a human in the loop).
    4. Shall: Provide basic workflow orchestration support.
    5. Should: Decide about the (degree of) support to be provided to the human, based on the respective situation and considering the experience of the human.
    6. Should: Support SWS between different AI modules.
    7. May: Facilitate customization to different pilot use cases via “configurable workflow definitions”.
3.8.7 Requirements towards other Components
    1. Shall: Components listed under “Inputs/Outputs” shall provide an API to allow Sliding Work Sharing to request/receive the required data
    2. Shall: Existing (legacy) systems or GUIs shall “act” based on the decisions/recommendations of the SWS Management component (i.e. request action/decision from human/AI/robot, provide transparency information to the human)
	

## 5. Evaluation Approach and Laboratory Prototypes

### 5.1 Key Performance Indicators 
AI4Work components will be evaluated at two levels of analysis. First, evaluation at an individual level focuses on the effectiveness of solutions at a resource level, considering technical components and human resources. Results from this level will assess the impact of the components on individual operational aspects, e.g., individual productivity of a technical component or a human operator, and on human aspects, e.g., well-being, stress, and job satisfaction. Second, evaluation at an organisational level focuses on the effectiveness of solutions in collaborative work settings, considering their tasks and processes from a system perspective. Again, a distinction is made between operational performance and human performance. Results from this level will assess the impact of the tools on organisational performance, e.g., team productivity, and on organisational aspects, e.g., team collaboration, coordination, and agility. Both levels of evaluation will contribute to developing a comprehensive and balanced understanding of the effectiveness of the AI4Work concept.
The evaluation will employ a mixed-method approach. Data will be collected using surveys, questionnaires, and wearables, while additional qualitative data will be gathered through e.g. interviews, focus groups, or co-creation activities. The data will be analysed using statistical methods and content analysis. Moreover, the evaluation will be conducted in close collaboration with the focal enterprises in follow-up co-creation activities [33]. The analysis results, at the first stage, will be used to refine the pilot applications and, at a subsequent stage, to develop guidelines for AI4Work components’ implementation in other organisations.
The initial set of KPIs has two measurement objectives: firstly, the monitoring of tool deployments and, secondly, the evaluation of their impact on work. The monitoring objective provides real-time feedback to system users and constitutes a core component of the interactive mechanisms for the active learning aspects of AI4Work components. In addition, developers and system designers will use historical monitoring data to improve system aspects with unsatisfactory user experience during the pilot phase. The evaluation objective is to assess the impact of AI4Work on individuals and organisations with an emphasis on the extrapolation of conclusions. Focusing on KPIs and assessment methodologies with pronounced generalizability traits serves as the basis for establishing best practices and guidelines for the future implementation of the AI4Work components in organisations and work environments across different sectors.
To effectively measure the impact of a component on the introduced KPIs, the evaluation will be conducted in two phases, namely pre- and post-deployment. The pre-deployment phase will assess the organisation’s original state in terms of the KPIs and establish a comparison benchmark for the impact assessment objective of the evaluation. The post-use phase measurements will define post-project operational targets for organisations to compare against pre-use benchmarks. Continuous post-use measurements will additionally be used to monitor the system and provide feedback to the developers. For pilot cases with sufficiently large application subject pools, control and treatment groups will be established to provide a more robust evaluation of the impact of each tool on various KPIs. The evaluation can use synthetic control groups [34] and event studies [35], [36] methodologies for cases with subject pools that do not permit forming control groups.
The remaining section presents the initial set of KPIs for monitoring and evaluating the AI4Work components. The KPIs will be customised for the project’s pilots in future deliverables of the project. Nonetheless, the section contains some early examples of potential uses of the introduced KPIs in the pilots. The KPIs are divided into three categories: (1) Technical KPIs, (2) Operational KPIs, and (3) Human-centric KPIs. Although KPIs for assessing the performance of AI-based SWS are scarce in previous literature (but cf. [37]), previous work and established standards on the evaluation of Cyber-Physical systems, AI decision support systems in manufacturing, and AI recommendation toolkits are used as a starting point. The KPIs concerning human-centricity are adjusted appropriately, including measurements of human users’ cognitive and emotional aspects and substitution effects during the work-sharing activities. Technical KPIs measuring AI efficiency, accuracy, and explainability are extended to aspects of AI-robot collaboration and overarching cyber-physical system performance, e.g., joint human-AI accuracy. Finally, operational KPIs and well-established econometric methods are used to assess the economic impact of AI4Work components on the production processes of the focal organisation units. Within each category, the KPIs are lexicographically ordered to facilitate future reference.
5.1.1 Technical KPIs
The technical KPIs are used to assess the performance of AI4Work components in terms of their technical capabilities. Compared to previous work, the novelty of employing the indicators of this category in the SWS context is that they are used to evaluate the efficiency, accuracy, and explainability of the AI solutions not only related to its technical resources but also in conjunction with human workers. The technical KPIs are relevant for measuring various aspects of SWS efficiency in all pilots.
    • Accuracy. Accuracy KPIs measure the degree to which an AI model generates a correct output for the problem given the available data [38], [39], [40]. Accuracy can be measured using different metrics, depending on the nature of the problem, e.g., classification, regression, or clustering. For classification problems, accuracy is often measured using metrics such as binary accuracy, categorical accuracy, sensitivity, specificity, false positive rate, precision, recall, F1-score, and ROC-AUC. For regression problems, metrics such as mean absolute error, mean squared error, and R-squared are used.
In specific quality applications, the classification accuracy metrics are relevant for evaluating the performance of the AI-human collaboration system as a single system. For example, in the product quality inspection activities of the manufacturing pilot, the classification accuracy metrics can be used to evaluate the human-AI collaborative inspection system as a single unit in a controlled environment, where the products not meeting the quality standards are ex-ante known. 
    • Latency. Latency measures a system’s responsiveness, typically in terms of the time it takes to process one unit of data [40]. In the AI4Work context, the latency concept can be extended to measure the system’s responsiveness in collaborative decision-making aspects, such as the interaction time until reaching a decision and the interaction time for providing corrections and feedback to the system.
    • Reliability. Reliability metrics evaluate whether the AI system behaves according to its design intentions [38], [40], [41]. Telemetry and usage statistics, such as the percentage of accepted or rejected suggestions, and user questionnaires querying the perceived quality of AI recommendations, can be used to measure SWS’ reliability.
    • Robustness. Robustness measures the ability of an AI system to perform in varying circumstances, e.g., in variations in the input data and context [38], [39], [40], [42], [43]. The robustness of the solutions can be assessed by the applicability of its components in various pilots.
    • Safety. Safety refers to a system’s capability to minimise unintentional harm stemming either from unexpected system actions or misuse of human operators [44]. SWS safety measures should primarily focus on assessing the safety of the CPS’s human actors. Relevant KPIs are recordings of safety incidents and questionnaires querying operators about their perceived safety. Safety and compliance measurements are relevant, e.g., for the logistics pilot.
    • Scalability. Scalability concerns a system’s ability to maintain its operational capacities in response to changes (typically up-scaling) of processing and input data demands [38], [45]. It is measured in terms of variability of performance metrics (e.g., processing time) as a function of varying complexity.
    • Security. Security refers to a system’s ability to prevent unintended execution, unauthorised access, and data leakage. The AI4Work components should identify and withstand external threats and adversarial attacks and be capable of maintaining information integrity. A system’s security attributes are measured using risk-based security metrics and penetration testing performance [38], [45], [46].
5.1.2 Operational KPIs
Operational KPIs are used to measure the performance of operational units. Extensive standardisation work on operational KPIs exists for example regarding manufacturing and automation [47], [48], [49], [50], [51]. We generalise aspects of the operational KPIs to render them applicable to a broader range of processes. In particular, the generalised KPIs are compatible with the operational contexts of AI4Work’s pilots and can be used when evaluating the contribution of the AI4Work tools in their operational environments. Modularity is a necessary condition for the evaluation of AI4Work’s contributions because, first, not all of the project’s components are uniformly applicable to all pilots and, second, those that are applicable to multiple pilots may affect the pilots’ workflows and processes, and even different tasks within a pilot’s process, in heterogeneous ways.
The introduction of AI4Work components affects operational units in three ways: (1) it introduces new tasks and processes, (2) it modifies existing tasks and processes, attributing to them human-AI collaboration characteristics, and (3) it makes certain tasks and processes obsolete. Operational KPIs can be used to assess the impact of AI4Work components on a task or process level and, in addition, to evaluate the overarching performance impact on operations in the application environment. In principle, such operational KPIs fall within the following listed categories, but will be required to be down-selected and customised to the needs of the pilots.
    • Cost/benefit ratios. The cost/benefit ratios are measures associating the costs relative to the benefits of a process and are indicative of the potential profit margins of operations. Because isolating the benefits of a component from other contributing factors is challenging, the impact of introducing an AI4Work component can, ceteris paribus, be approximated by the difference between the cost/benefit ratio of the process before and after the introduction of the system. Cost/benefit ratios are relevant, e.g., for logistics, manufacturing, construction, and agriculture pilots.
    • Cycle time. Cycle time KPIs measure the elapsed time between initiating and completing a task, process, or machine operation. Cycle times can measure the performance of the entire CPS and separate AI4Work components while operating autonomously. Manufacturing lead times measure the time between initiating and completing a process; for instance, a human-AI decision-making interaction about task sharing. Order processing times measure the time between initiation and completion of an order. Machine set-up times measure the time required to (re-)configure a machine. Such measurements can be helpful in the yard logistics and manufacturing pilots.
    • Delivery reliability. Delivery reliability KPIs measure a supplier’s capacity to serve customers on time. For example, reliability is calculated as the percentage of on-time deliveries in manufacturing systems. Monitoring such KPIs can be used to assess whether the introduction of AI4Work modules renders the delivery process more reliable. These KPIs are relevant, e.g., for the manufacturing pilot.
    • Deployment costs. The cost of deploying AI4Work components in an operational environment. Deployment costs include the costs of material, energy, and labour (including training) required for the system to be operational.
    • Environmental sustainability. Environmental sustainability metrics measure the environmental impact of a process. They are usually measured in terms of energy consumption, waste production, and the system’s carbon footprint (CO2 emissions). Measuring the system’s environmental impact is not strictly relevant to AI4Work pilots, although it can be broadly considered as a relevant criterion beyond the pilots.
    • Perceived quality. Perceived quality metrics measure the perceived satisfaction with a process’ output. In the AI4Work context, the perceived quality of the AI or SWS output can be measured by surveying operators (and end-consumers whenever relevant) about their satisfaction with the system’s output. Such measurements can be performed for output produced either while the AI-enabled system was operating autonomously or in collaboration with human actors.
    • Costs. Cost metrics measure the costs associated with a process. The costs relating to the deployed AI4Work components can be measured in terms of energy consumption, maintenance costs, and the (re-)configuration costs. For example, cost savings on storage and handling are relevant for the logistics pilot.
    • Flexibility. Flexibility KPIs capture the degree to which processes can be modified to address changes in operational requirements. The flexibility of AI4Work components can be measured in terms of re-configuration or set-up time required for a component to adjust to alternative context requirements.
    • Productivity. Productivity metrics measure the efficiency of a process. Usually, labour and (physical) capital are considered factors. The combined efficiency of the process is total productivity. Productivity is typically measured in terms of output units per unit (or batch) of input. Productivity metrics are relevant for the logistics, manufacturing, construction, and agriculture pilots.
    • Quality. Quality metrics measure the degree to which a process meets the demanded specifications or requirements. The quality of AI4Work tools can be measured by the amount of unsatisfactory output while the system operates either autonomously or in conjunction with human actors. Quality KPIs are relevant for manufacturing, construction, education, and healthcare pilots.
    • Return on investment. Return on investment is an investment’s profitability measure. The return on investment of the system can be measured as the difference between the initial cost of introducing the AI4Work components in a process and the resulting expected discounted future marginal benefits. For instance, return on investment is relevant for the logistics, manufacturing, construction, and agriculture pilots. However, its measurement is less relevant within the project lifecycle and is more relevant in potential investment decisions afterwards. 
    • Routing flexibility. Routing flexibility measures assess the capability of a process to complete an operation in multiple ways. In a traditional manufacturing environment, routing flexibility is measured as the number of ways a part type can be produced. In the AI4Work context, routing flexibility can be measured as the number of proposed ways (allocations) a task can be shared between human and AI actors.
5.1.3 Human-centricity KPIs
Human-centric KPIs primarily measure qualitative and quantitative aspects of work design from a human involvement perspective at the individual and organisational levels. They are relevant for measuring both blue- and white-collar work aspects [52].
    • Agency. Agency metrics traditionally measure the discretion of human agents when making decisions to carry out work (human agency). Agency can be measured via qualitative user assessments, e.g., questionnaires, interviews, and focus groups [53], [54], [55], [56], [57], [58]. For CPS with AI agents exhibiting autonomous behaviour, pre-application agency metrics and post-application operators’ surveys can be used to evaluate the ability of AI actors to take over tasks requiring decision-making autonomy (technical agency).
    • AI Acceptance. AI acceptance measurements quantify the degree to which human actors (usually workers) accept the AI agents’ presence in a work environment. Acceptance can be measured using questionnaires, interviews, and focus groups. In the AI4Work context, AI acceptance measurements can monitor and assess the integration of AI4Work’s components in work processes. Telemetry and usage statistics can be used to monitor the integration of the components across time. These measurements can be indicative of how acceptance of AI4Work solutions evolves as familiarity with the introduced components increases.
    • Ease of use. Ease of use metrics measure the accessibility of a system’s functionalities to a user and the ease of learning to use the system effectively and efficiently. Qualitative user assessment and usage statistics are used to measure the ease of use [40] [8]. For the AI4Work case, ease of use metrics can be specialised to a module level, for instance, to the ease of use of the AI-human collaboration interface.
    • Explainability. Explainability refers to the capacity of a system to provide humans with comprehensible explanations of its decisions and the rationale of its actions (outcome-centric understandability) [40], [59], [60]. It can be measured using qualitative user assessments [61]. In addition, it can be indirectly measured by usage statistics, e.g., the frequency of users requesting (additional) explanations of the system’s decisions, and the time spent by users with explainability UI components [62]. Explainability metrics, for example, are particularly relevant for the logistics and manufacturing pilots.
    • Fairness. Fairness assessments quantify a system’s non-discrimination capabilities. Fairness KPIs concern responsible data acquisition and management, design fairness, and bias prevention in outcomes [54], [57], [59], [61], [63], [64].
    • Feedback from job/others. Feedback metrics measure the degree of feedback provided to a worker in an operational work environment [55]. In the AI4Work context, user statistics and telemetry can measure feedback from human to AI agents. In the reverse direction, explainability KPIs measure feedback from AI to human agents [53], [54], [65].
    • Inclusivity. Inclusivity KPIs quantify the practice of providing equal access to opportunities and resources for users, especially users belonging to minority groups or traditionally misrepresented groups [54], [63], [64], [65], [66]. Questionnaires and other qualitative user assessment techniques can be used to measure perceived inclusivity. Additionally, differences in telemetry and usage statistics between different user groups (e.g., males and females) can be indicative of inclusivity issues.
    • Information processing needs. Information processing KPIs quantify the requirements of interpreting, gathering, and synthesising information for decision-making purposes. Qualitative user assessments are used for human actors [55]. Throughput, i.e., the number of tasks or transactions the AI system can process in a given time frame, can be used to measure the information processing needs of tasks taken over by AI agents [53], [54], [65].
    • Interdependence. Interdependence KPIs measure the degree to which process (human) actors depend on each other [54], [55], [67]. Interdependence measurements are collected via questionnaires, interviews, and focus groups [53], [67]. These KPIs can be extended to measure interdependence between all actors in CPS processes. In the AI4Work context, questionnaires should primarily focus on the interdependence between human and AI actors.
    • Interpretability. Interpretability metrics measure the ability to translate a system’s working principles into human-understandable language (model design-centric understandability). It can be measured via user surveys [6], [40], [46], [59].
    • Mental demands. Mental demands measurements quantify the cognitive costs associated with performing a task. Mental demand KPIs include measurements regarding how stimulating, challenging, and diverse a task is for the human workers of a work process [68], [69]. These aspects can be measured using questionnaires and surveys for human workers. Mental demands KPIs do not apply to AI agents. Nonetheless, aspects of task processing requirements are captured by the latency (technical) and process costs (operational) KPIs [54]. Indirect estimates of mental demand measurements for tasks taken over by AI agents can be obtained by surveying human agents on these tasks’ mental requirements before the AI4Work components’ introduction [53]. These proxies can be relevant when analysing work substitution effects between AI and human agents.
    • Physical demands. Physical demands measurements quantify the physical efforts associated with performing a work task [53], [54], [55]. Physical demand KPIs can be measured using wearable devices and questionnaires for human workers. Similar to mental demands, physical demands KPIs are not applicable to AI agents. However, for cases where work substitution effects are of interest, aspects concerning the physicality of tasks performed by robot agents can be obtained by surveying human agents on the physical requirements of these tasks before introducing the AI4Work tools. For example, physical demand measurements are relevant for the agriculture and construction pilots.
    • Physical ergonomics. Physical ergonomics KPIs reflect how a task or process allows workers to maintain correct postures and proper movements. These KPIs are less relevant for AI agents. Qualitative user assessments are used to measure physical ergonomics for human actors [53], [54], [55], [70], [71]. In addition, absenteeism due to health issues and records of in-work injuries can be indicative proxies for indirectly measuring the physical ergonomic aspects of specific tasks.
    • Privacy. Privacy KPIs measure a system’s capacity to ensure informational privacy [38], [46], [54]. For SWS environments, such KPIs can measure the system’s ability to respect the workers’ private sphere and the right to control the dissemination of their data. Privacy aspects of the SWS system can be measured by recording incidents of unintended information leakage. Further, perceived privacy can be measured via qualitative user assessments. Privacy KPIs are relevant, e.g., for the healthcare and education pilots.
    • Problem-solving. Problem-solving KPIs assess the degree of innovativeness required for completing a task. Qualitative user assessments can be used to measure the perceived problem-solving requirements of a task for humans. Pre-SWS application measurements can indirectly assess the AI agents’ problem-solving capacities from a human perspective [53], [54].
    • Satisfaction. Satisfaction KPIs measure an operator’s contentment with a process’ performance. Satisfaction can be measured using questionnaires, interviews, and focus groups [40]. In the AI4Work context, satisfaction metrics can be specialised for each pilot scenario and each deployed module.
    • Skill variety. Skill variety metrics measure the range of skills needed for a task or process [54], [55]. Skill variety is measured via qualitative user assessments for human agents. Variety metrics can be relevant for measuring the versatility of tasks performed by the AI agents of the SWS system [53]. From the human operators’ perspective, the perceived skill variety of AI components can be measured using questionnaires.
    • Stress. Stress KPIs measure the degree of mental or emotional strain resulting from demanding work circumstances. Qualitative user assessment and wearable devices are used to measure stress levels [68], [69]. Measuring stress via wearable devices uses physiological measurements, e.g., heart rate variability, skin conductance, and cortisol levels. Furthermore, stress KPIs can be measured around time windows of human-AI interactions to assess aspects of the SWS system acceptance from the human actors’ perspective. State-Trait Anxiety Inventory (STAI) and Perceived Stress Scale (PSS) measurements are relevant for the healthcare and education pilots.
    • Task variety. Task variety KPIs quantify the variability and range of performed tasks [53], [54], [55], [57]. In the context of AI4Work, task variety KPIs should focus on the range of tasks performed by human agents because the technical robustness and operational flexibility KPIs cover the aspects relating to AI agents. Task variety traits can be measured by qualitative user assessment.
    • Work conditions. Work condition KPIs assess the appropriateness and appeal of the environment where a task is performed. They include measurements regarding health hazards, noise, temperature, and cleanliness [53], [54], [55] and are typically measured by questions based on work design questionnaires.

### 5.8 Sliding Work Sharing
5.8.1 Key Features and Test Data 
The aim of the laboratory prototype for the SWS Management component was to do early experiments regarding the core functionality of the SWS, i.e. the sliding module that shall dynamically decide about the degree of AI autonomy vs. the degree of human involvement. As this decision is very much dependent on the concrete application scenario, the SWS Management needs to be flexibly adaptable to pilot-/use-case-specific rules. Besides, it is expected that those rules will be formulated in a rather vague way in the beginning, afterwards requiring to elaborate the meaning of rules in more detail and fine-tune the effects based on pilot tests.
Considering these goals, an SWS Management approach based on fuzzy logic seemed promising for the laboratory prototype. The library jFuzzyLogic [77] [78] was chosen to do initial experiments. It implements the Fuzzy Control Language (FCL) and provided an initial demo scenario (including visualisation) that could be easily adapted to fit with the basic concept of the SWS Management component. 
An experimental example is visualised in Figure 39, where the values of the two variables “ai confidence level” and “human experience level” are used as input to calculate the “suggested work sharing approach”.

Figure 39: Visualization of the SWS Management Lab Prototype
This experiment was done based on an abstract test data set that is not related to any pilot. The values for the two input variables are indicated on a scale from 0.0 to 10.0, the value for the output variable can be between 0.0 and 4.0. These ranges were randomly chosen and can be flexibly adapted for a concrete pilot application scenario. 
The membership functions, indicated by the coloured areas, define the degree of membership in the fuzzy sets low/medium/high for each input value. They are defined via the FCL, for example:
FUZZIFY human_experience_level
   TERM low := (0, 1) (2, 1) (4,0);
   TERM medium := (2, 0) (4,1) (5,1) (7,0);
   TERM high := (5, 0) (7, 1) (10, 1);
END_FUZZIFY
After translating the input values via the membership functions (the so-called “fuzzification”), fuzzy rules like the following can be applied:
IF ai_confidence IS low 	THEN suggested_work_sharing_approach IS human_manually;
IF ai_confidence IS high 	THEN suggested_work_sharing_approach IS ai_autonomously;
IF ai_confidence IS medium AND human_experience IS medium 	
THEN suggested_work_sharing_approach IS human_in_the_loop;
IF ai_confidence IS medium AND human_experience IS low 	
THEN suggested_work_sharing_approach IS human_on_the_loop;
The application of all applicable fuzzy rules and the subsequent “de-fuzzification” results in a numerical value for the “suggested work sharing approach” (in this example it starts from “human doing the work manually” via “human in/on the loop” up to the “AI working autonomously”). 
5.8.2 Proof-of-Concept Results
The chosen laboratory prototype approach seems to provide enough flexibility for application in a variety of pilot use cases. This requires that pilot-specific data has to be translated/mapped to numerical inputs/outputs of the fuzzy engine, i.e. the following example inputs/outputs of the lab prototype may have different meaning depending on the use case: 
human experience level: 4.70,	ai confidence level:7.10 => suggested work sharing level: 2.60
human experience level: 6.10,	ai confidence level:4.00 => suggested work sharing level: 0.95
human experience level: 7.60,	ai confidence level:8.60 => suggested work sharing level: 3.50
Both input and output variables can be defined as numbers on a self-chosen scale, thus providing flexibility when customising this to different use cases. Besides, also the fuzzy rules can be customised, and the effects of the rules can be fine-tuned via the membership functions.
The results of the laboratory tests show that fuzzy logic is a promising approach to implement the core functionality of the SWS Management component. It can be flexibly customised via pilot-specific rules and variables, thus making it applicable in all AI4Work pilots.



# AI4Work-D5.1 (2025) - Pilots plan - Initial Version
## 4. Pilot 2 – Manufacturing
4.1 Pilot Introduction
The e-commerce sector has experienced rapid growth, prompting medium to large-sized companies to adopt automation technologies and enhance human resource capabilities to meet increasing demands in procurement, packaging, and delivery operations. These enterprises aim to boost the efficiency of their packing lines through advanced automation while also recruiting personnel ranging from machine operators to line supervisors and production managers. However, recruitment poses challenges due to a shortage of specialized workers and the monotonous nature of many machine-related tasks, leading to high turnover rates, limited employee expertise, and unclear role definitions. To address these gaps, highly automated systems are being introduced to reduce the need for manual operations.
Yet, the shift to advanced automated equipment introduces a new set of challenges compared to traditional manufacturing. Operators and maintainers must now manage unforeseen situations and devise solutions for complex issues, while production managers face increased difficulty in coordinating plant activities and aligning workforce skills with operational needs. Automated packing technologies can simplify workers' tasks, but they also demand a higher level of problem-solving and planning expertise.
In this context, artificial intelligence (AI) can play a pivotal role in supporting inexperienced operators and assisting managers in decision-making. The AI4Work initiative, both during and after its implementation, aims to address packaging challenges by focusing on three key areas: troubleshooting unexpected failures, guiding operators in routine tasks, and optimizing plant activity planning to improve productivity. Although these objectives may appear straightforward or align with existing practices, AI4Work envisions innovative methods to effectively balance human resources with AI and robotic systems.
This approach aligns seamlessly with the challenges identified. While human labour remains a vital resource for plant managers, its availability and skill level are often inconsistent and insufficient. By integrating AI with tools like digital twins, companies can enhance collaboration between humans and machines, streamline human-machine interactions, and develop optimal strategies for scheduling tasks, taking into account employees’ skills and availability.
4.2 Description of Use Cases
4.2.1 UC-MAN-1: Troubleshooting
Objective:
To address the high turnover rate among machine operators, which hinders the development of in-depth expertise, a Digital Assistant is proposed. This tool will leverage accumulated knowledge from past operator experiences to offer real-time guidance during troubleshooting activities.
Scenario:
When a fault occurs in an E-CO Flex machine that exceeds the expertise of the maintenance operator, a maintenance ticket is created. This ticket captures information about the machine’s status, production data, and sensor readings. The Digital Assistant analyses these inputs, combines them with historical fault data, and recommends optimal troubleshooting steps. Historical data may include prior tickets with fault resolutions, enabling operators to learn from past scenarios.
Key Requirements:
    • A troubleshooting tool must analyse faults and provide recommendations using machine status and historical data (Shall).
    • It should identify new or unknown faults not yet present in the database (Should).
    • A digital twin should simulate machine behaviour, enabling fault association with operating conditions (May).
4.2.2 UC-MAN-2 Training
Objective:
To provide machine operators with effective, customized training that reduces learning curves, minimizes errors, and enhances productivity through an interactive virtual assistant.
Scenario:
Operators, aiming to improve their skills in machine operations and maintenance, access a virtual assistant. This assistant uses natural language processing to extract information from operating manuals and offers step-by-step guidance tailored to the operator’s skill level. It supports users in understanding common machine procedures, making onboarding smoother and more efficient.
Key Requirements:
    • The virtual assistant must facilitate training activities and support operators in developing expertise (Shall).
    • It must accept and respond to natural language queries to ease user interaction (Shall).
    • Answers provided should consider the user’s skill level to tailor responses effectively (May).
    • The assistant may also identify machine conditions requiring human intervention to offer proactive support (May).
4.2.3 UC-MAN-3: Activity Scheduling
Objective:
To optimize production line operations by automating scheduling processes while considering human and machine resources, raw material availability, and operational constraints.
Scenario:
AI4Work processes input orders specifying product details, urgency levels, and resource availability. It generates dynamic production line plans, balancing workloads between human operators and machines. Line managers can introduce new constraints, such as resource shortages or skill mismatches, and the system adjusts plans in real-time to maintain efficiency.
Key Requirements:
    • The tool must generate automated production plans that optimize resource utilization (Shall).
    • It should consider constraints like raw material availability and human resources (Shall).
    • Plans must dynamically adapt to workload balancing requests (Shall).
    • Digital twins should simulate production line scenarios, modelling both human and machine performance (Shall).
4.3 Plan for Integration of Technologies
The "AI4Work" initiative leverages a robust suite of advanced technologies to meet the specific objectives outlined in its three core use cases. This chapter outlines the integration of these technologies within the systems developed for Troubleshooting, Training, and Activity Scheduling, ensuring a cohesive approach to addressing operational challenges in the e-commerce packaging environment. The following sections detail the integration for each use case.
4.3.1 UC-MAN-1: Troubleshooting
The Troubleshooting system combines several advanced technologies to create a cohesive solution:
    • Data Collection and Handling for AI/Robotics Services: Collects and processes real-time machine data to identify and analyse faults.
    • Human-Centred Digital Twins: Simulates machine behaviour and past fault conditions, allowing operators to visualize and test solutions virtually.
    • Context Awareness: Adapts troubleshooting recommendations based on the machine's current operating context and historical data.
    • Sliding Work Sharing: Enables flexible collaboration between humans and machines, allowing operators to focus on high-priority issues while AI manages routine troubleshooting.
By integrating these technologies, the system empowers operators with enhanced diagnostic tools, reduces downtime, and bridges the gap in expertise caused by workforce turnover.

Figure 3. AI4Work Architecture adapted for the EP of the Manufacturing Pilot – Use Case 1
4.3.2 UC-MAN-2: Training
The Training system incorporates the following technologies to achieve its goals:
    • Data Collection and Handling for AI/Robotics Services: Centralizes training data from operator manuals and real-time machine usage to create adaptive training modules.
    • Pilot-Specific Digital Twin: Provides virtual environments where operators can practice procedures and interact with simulated machines.
    • Context Awareness: Personalized training content to match the operator's skill level and the machine's current state.
    • Sliding Work Sharing: Allows the system to distribute training tasks dynamically, balancing human learning efforts with automated guidance.
    • Natural Language Processing: Facilitates intuitive, conversational interactions between operators and the virtual training assistant.
    • Machine HMI: Hardware/software component that enables humans to interact with the primary functions of the packaging machine.
This integrated approach ensures that operators can quickly gain the expertise needed to operate complex machinery effectively while minimizing errors during training. 

Figure 4. AI4Work Architecture adapted for the EP of the Manufacturing Pilot – Use Case 2
4.3.3 UC-MAN-3: Activity Scheduling
The Activity Scheduling system employs the following technologies:
    • Data Collection and Handling for AI/Robotics Services: Gathers data on resource availability, production demands, and operational constraints.
    • Human-Centred Digital Twins: Simulates production scenarios including human operators to test and refine schedules before implementation.
    • Human-Aware Task Planning: Plan the workflow of operators and machines, considering human expertise and availability.
    • Context Awareness: Is aware of a context change, such as machine status, order priorities, and resource availability, and sends this context change in real-time to the Sliding Work Sharing building block. 
    • Sliding Work Sharing: Ensures flexible task allocation, enabling efficient collaboration between human operators and automated systems. It eventually adjusts the schedule or triggers the Human-Aware Task Planning for rescheduling.
Through this integration, the system enables line managers to optimize resource utilization and maintain high productivity while responding dynamically to operational challenges. 

Figure 5. AI4Work Architecture adapted for the EP of the Manufacturing Pilot – Use Case 3
The integration of advanced technologies across all three use cases ensures that AI4Work delivers comprehensive, scalable solutions to address the unique challenges of e-commerce packaging operations. By leveraging these technologies in tandem, the project enhances operational efficiency, supports workforce adaptability, and enables seamless collaboration between humans and intelligent systems.
4.4 Initial Plan – Early Prototypes
4.4.1 UC-MAN-1: Troubleshooting
O1.1 Define data model for 100% of available analytics [December ‘24]. Machine records and production statistics, including context and troubleshooting, are produced in a variety of formats. In order to populate a proper digital experience repository, it is necessary to define a common data model for this information.
O1.2 Populate the experience storage with 500 entries [March ‘25]. Experience data are collected from the field, although in the early stages of development, in the absence of full-scale installations, most historical data is generated in protected environments or laboratory setups. Collected entries may or may not include faulty conditions that require troubleshooting. A significant part of them, however, must be suitable for training and use case proofing.
O1.3 Assistance success rate of 80% in a virtual environment [June ‘25]. At the end of the early prototype stage, the quality of collected information must be sufficient to provide evidence of successful troubleshooting in the 80% of the fault conditions recorded. More detailed metrics will be defined to assess the model performance.
4.4.2 UC-MAN-2: Training
O2.1 Supervised service training for 50 situations [December ‘24]. In an initial phase, the specific AI will be trained using the troubleshooting manuals of the machines concerned. This specific training process will enable the system to gain an in-depth understanding of at least 50 technical issues and applicable solutions to restore the plant operational efficiency.
O2.2 Assistance success rate of 80% in a virtual environment [March ‘25]. In the event of an alarm, the machine stops, and the specific AI activates its training support function. Through the machine interface, the system provides the operator with detailed advice on how to solve the problem. During the system validation phase, an experienced machine operator evaluates the proposed solution. A positive evaluation rate of 80% is expected.
O2.3 Replicate previous goals adopting continuous training [June ‘25]. During the training process, the specific AI will store the actions performed by the operator that led to the actual resolution of the problem through a semantic representation. This information will be integrated into the system's knowledge base, enabling the specific AI to constantly improve and provide up-to-date, accurate and reliable solutions.
4.4.3 UC-MAN-3: Activity Scheduling
O3.1 Optimise a 1-obj. problem involving 1 line, N packaging operators, 1 replenishment operator [December ‘24]. The Early Prototype aims to schedule the replenishment of a packaging line, which includes one E-CO Flex machine, N packaging operators (with N ≥ 2) and one replenishment operator. Replenishment refers to restocking the drawers of the machine. This activity is limited to specific designated slots and can be performed even when the drawers are not completely empty. The optimization metric aims to minimize the number of replenishment actions.
O3.2 Implement 3D DES interfacing with O3.1 assessing plan execution time [June ‘25]. The Early Prototype aims to simulate a single line with N packaging bays and one E-CO Flex machine. The dispatching of boxes is based on a threshold-based rule, where a specific box size is assigned to each bay. The main objective of the EP is testing the integration with data coming from IMA (CAD and designs) and FBK (production plans), estimating the execution time and testing the replenishment policy. The model provides operators’ saturation and machines’ states.
4.5 Prototype Iteration Cycle
The development of the AI4Work prototypes follows an iterative methodology emphasizing a collaborative interaction between technology providers and the end-user (IMA). Each iteration involves testing, feedback collection, and refinement, ensuring that the prototypes progressively align with operational needs. The process begins with early prototypes deployed in controlled environments, such as virtual or testbed setups, where proxy operators (i.e. IMA technicians) perform predefined tasks while closely monitored by the development team. Feedback from these sessions informs adjustments in the system's functionality, user interface, and integration with existing workflows.
Subsequent iterations involve deploying improved prototypes in real machine scenarios. During these phases, operators interact with the prototypes in industrially relevant environments, while technology providers gather real-world performance data and feedback. This iterative loop continues until the final prototype meets the desired performance and usability metrics.
The iterative process leverages key mechanisms, such as digital twins and context-aware systems, to simulate and anticipate the impact of changes before implementation. By engaging R&D and business unit staff throughout, this approach ensures the solutions are industry-driven, human-centred, and scalable.
Table 2. Prototype iteration cycle of the Manufacturing Pilot
Stage	Phase	Environment	Activities	Outcomes
Early prototype	Testing	Virtual or testbed	IMA technicians perform predefined tasks (e.g. start, stop, daily maintenance, packing trials, well-known fault solving procedures)	Identified areas for improvement (e.g. weak points, critical usability issues, missing features)
	Feedback collection	Virtual or testbed	Technology developers monitor and collect feedback	
	Refinement	Virtual or testbed	Technology developers refine algorithms based on user interactions	Enhanced algorithms and user interfaces (e.g. stability, responsiveness, performance)
Final prototype	Testing	Industrially relevant	IMA technicians use prototypes in daily tasks and complex scenarios (e.g. recovery procedures from severe faults, stress tests with variable workloads, handling of unexpected situations/ requests)	Identified improvements tailored to real-world needs (e.g. difficulty to adapt to specific situations, poor performance)
	Feedback collection	Industrially relevant	Technology developers monitor and collect feedback	
	Refinement	Industrially relevant	Developers apply refinements to optimise prototype performance and reliability in actual workflows	Ready-to-deploy prototypes with validated performance (e.g. succeeding on-field testing protocols)

