{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rdflib pandas rapidfuzz requests pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803657c",
   "metadata": {},
   "source": [
    "Cell 0 — Setup (install & imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc1fa732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready.\n"
     ]
    }
   ],
   "source": [
    "# If needed, uncomment to install:\n",
    "# !pip install rdflib pandas rapidfuzz requests pyyaml\n",
    "\n",
    "import os, json, math, re, uuid, time, pathlib, textwrap\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import pandas as pd\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, URIRef\n",
    "from rapidfuzz import process, fuzz\n",
    "import numpy as np\n",
    "\n",
    "print(\"Imports ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ac1aa",
   "metadata": {},
   "source": [
    "Cell 1 — Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7b88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config set. Edit TTL_PATH and DOC_MD_PATH before running.\n"
     ]
    }
   ],
   "source": [
    "# --- REQUIRED: set your file paths here ---\n",
    "TTL_PATH = r\"sws.ttl\"     # e.g., r\"C:\\projects\\base.ttl\"\n",
    "DOC_MD_PATH = r\"swsmd.md\"        # e.g., r\"C:\\projects\\usecase.md\"\n",
    "\n",
    "# Ollama local model (e.g., 'llama3.1:8b', 'qwen2.5:7b', etc.)\n",
    "OLLAMA_MODEL = \"mistral\"\n",
    "\n",
    "# Choose \"flat_json\" (default, simpler) or \"jsonld\"\n",
    "MAPPING_FORMAT = \"flat_json\"\n",
    "\n",
    "# Embedding settings\n",
    "USE_EMBEDDINGS = True                 # turn off to fall back to fuzzy-only\n",
    "OLLAMA_EMBED_MODEL = \"nomic-embed-text\"\n",
    "EMBED_TOPK = 5                        # how many nearest ontology terms to consider\n",
    "EMBED_MIN = 0.75                      # cosine threshold to accept a match directly\n",
    "COMBO_ALPHA = 0.65                    # blend: alpha*cosine + (1-alpha)*fuzzy\n",
    "COMBO_MIN = 0.80                      # accept if blended score >= this\n",
    "EMBED_BATCH = 64                      # batch size for embedding ontology surface forms\n",
    "\n",
    "\n",
    "# Output directory\n",
    "OUT_DIR = \"./outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Config set. Edit TTL_PATH and DOC_MD_PATH before running.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e6f7c",
   "metadata": {},
   "source": [
    "Cell 2 — Load ontology & build lookup index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271143af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71 classes, 26 object properties, 13 data properties.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kind</th>\n",
       "      <th>iri</th>\n",
       "      <th>qname</th>\n",
       "      <th>label</th>\n",
       "      <th>alt_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>class</td>\n",
       "      <td>http://data.europa.eu/esco/model#Skill</td>\n",
       "      <td>esco:Skill</td>\n",
       "      <td>Skill</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>class</td>\n",
       "      <td>http://www.sws.org/sws#AIAcceptance</td>\n",
       "      <td>:AIAcceptance</td>\n",
       "      <td>AIAcceptance</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>class</td>\n",
       "      <td>http://www.sws.org/sws#Accuracy</td>\n",
       "      <td>:Accuracy</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class</td>\n",
       "      <td>http://www.sws.org/sws#Actor</td>\n",
       "      <td>:Actor</td>\n",
       "      <td>Actor</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>class</td>\n",
       "      <td>http://www.sws.org/sws#ActorStatus</td>\n",
       "      <td>:ActorStatus</td>\n",
       "      <td>ActorStatus</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>data_property</td>\n",
       "      <td>http://www.sws.org/sws#hasSkillLevel</td>\n",
       "      <td>:hasSkillLevel</td>\n",
       "      <td>hasSkillLevel</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>data_property</td>\n",
       "      <td>http://www.sws.org/sws#hasSocialPerformanceScore</td>\n",
       "      <td>:hasSocialPerformanceScore</td>\n",
       "      <td>hasSocialPerformanceScore</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>data_property</td>\n",
       "      <td>http://www.sws.org/sws#hasTaskComplexity</td>\n",
       "      <td>:hasTaskComplexity</td>\n",
       "      <td>hasTaskComplexity</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>data_property</td>\n",
       "      <td>http://www.sws.org/sws#hasTaskStatusDescriptor</td>\n",
       "      <td>:hasTaskStatusDescriptor</td>\n",
       "      <td>hasTaskStatusDescriptor</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>data_property</td>\n",
       "      <td>http://www.sws.org/sws#hasTechnicalPerformance...</td>\n",
       "      <td>:hasTechnicalPerformanceScore</td>\n",
       "      <td>hasTechnicalPerformanceScore</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              kind                                                iri  \\\n",
       "0            class             http://data.europa.eu/esco/model#Skill   \n",
       "1            class                http://www.sws.org/sws#AIAcceptance   \n",
       "2            class                    http://www.sws.org/sws#Accuracy   \n",
       "3            class                       http://www.sws.org/sws#Actor   \n",
       "4            class                 http://www.sws.org/sws#ActorStatus   \n",
       "..             ...                                                ...   \n",
       "105  data_property               http://www.sws.org/sws#hasSkillLevel   \n",
       "106  data_property   http://www.sws.org/sws#hasSocialPerformanceScore   \n",
       "107  data_property           http://www.sws.org/sws#hasTaskComplexity   \n",
       "108  data_property     http://www.sws.org/sws#hasTaskStatusDescriptor   \n",
       "109  data_property  http://www.sws.org/sws#hasTechnicalPerformance...   \n",
       "\n",
       "                             qname                         label alt_labels  \n",
       "0                       esco:Skill                         Skill         []  \n",
       "1                    :AIAcceptance                  AIAcceptance         []  \n",
       "2                        :Accuracy                      Accuracy         []  \n",
       "3                           :Actor                         Actor         []  \n",
       "4                     :ActorStatus                   ActorStatus         []  \n",
       "..                             ...                           ...        ...  \n",
       "105                 :hasSkillLevel                 hasSkillLevel         []  \n",
       "106     :hasSocialPerformanceScore     hasSocialPerformanceScore         []  \n",
       "107             :hasTaskComplexity             hasTaskComplexity         []  \n",
       "108       :hasTaskStatusDescriptor       hasTaskStatusDescriptor         []  \n",
       "109  :hasTechnicalPerformanceScore  hasTechnicalPerformanceScore         []  \n",
       "\n",
       "[110 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph()\n",
    "g.parse(TTL_PATH, format=\"turtle\")\n",
    "\n",
    "def qname_safe(uri: URIRef) -> str:\n",
    "    try:\n",
    "        return g.namespace_manager.normalizeUri(uri)\n",
    "    except Exception:\n",
    "        return str(uri)\n",
    "\n",
    "def collect_terms(graph: Graph):\n",
    "    classes = sorted(set(graph.subjects(RDF.type, OWL.Class)))\n",
    "    obj_props = sorted(set(graph.subjects(RDF.type, OWL.ObjectProperty)))\n",
    "    data_props = sorted(set(graph.subjects(RDF.type, OWL.DatatypeProperty)))\n",
    "    return classes, obj_props, data_props\n",
    "\n",
    "def get_label(graph: Graph, term: URIRef) -> str:\n",
    "    lab = graph.value(term, RDFS.label)\n",
    "    return str(lab) if lab else qname_safe(term)\n",
    "\n",
    "classes, obj_props, data_props = collect_terms(g)\n",
    "\n",
    "def build_index(graph: Graph, terms: List[URIRef], kind: str):\n",
    "    rows = []\n",
    "    for t in terms:\n",
    "        row = {\n",
    "            \"kind\": kind,\n",
    "            \"iri\": str(t),\n",
    "            \"qname\": qname_safe(t),\n",
    "            \"label\": get_label(graph, t),\n",
    "            \"alt_labels\": [str(o) for o in graph.objects(t, SKOS.altLabel)]\n",
    "        }\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "rows = []\n",
    "rows += build_index(g, classes, \"class\")\n",
    "rows += build_index(g, obj_props, \"object_property\")\n",
    "rows += build_index(g, data_props, \"data_property\")\n",
    "\n",
    "onto_df = pd.DataFrame(rows).fillna(\"\")\n",
    "print(f\"Loaded {len(classes)} classes, {len(obj_props)} object properties, {len(data_props)} data properties.\")\n",
    "onto_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ontology signatures and subclass helpers ---\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# subclass parents: qname(child) -> set(qname(parent))\n",
    "_sub_parents = defaultdict(set)\n",
    "for c in classes:\n",
    "    for sup in g.objects(c, RDFS.subClassOf):\n",
    "        if isinstance(sup, URIRef):\n",
    "            _sub_parents[qname_safe(c)].add(qname_safe(sup))\n",
    "\n",
    "def is_subclass_or_equal(cls_qn: str, sup_qn: str) -> bool:\n",
    "    if cls_qn == sup_qn:\n",
    "        return True\n",
    "    seen = {cls_qn}\n",
    "    dq = deque([cls_qn])\n",
    "    while dq:\n",
    "        x = dq.popleft()\n",
    "        for p in _sub_parents.get(x, ()):\n",
    "            if p == sup_qn:\n",
    "                return True\n",
    "            if p not in seen:\n",
    "                seen.add(p); dq.append(p)\n",
    "    return False\n",
    "\n",
    "# property signatures: qname(prop) -> {domain: set(qname), range: set(qname or xsd types)}\n",
    "prop_sig = {}\n",
    "def _qset(vals):\n",
    "    out = set()\n",
    "    for v in vals:\n",
    "        if isinstance(v, URIRef):\n",
    "            out.add(qname_safe(v))\n",
    "    return out\n",
    "\n",
    "for p in list(obj_props) + list(data_props):\n",
    "    pq = qname_safe(p)\n",
    "    dom = _qset(g.objects(p, RDFS.domain)) or {\"owl:Thing\"}\n",
    "    rng = _qset(g.objects(p, RDFS.range))   # may be classes or xsd types\n",
    "    prop_sig[pq] = {\"domain\": dom, \"range\": rng}\n",
    "\n",
    "def domain_matches(prop_qn: str, cls_qn: str) -> bool:\n",
    "    doms = prop_sig.get(prop_qn, {}).get(\"domain\", {\"owl:Thing\"})\n",
    "    if \"owl:Thing\" in doms:\n",
    "        return True\n",
    "    return any(is_subclass_or_equal(cls_qn, d) for d in doms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682ad43",
   "metadata": {},
   "source": [
    "Cell 3 — Build fuzzy matchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b97a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space built with 220 surface forms.\n"
     ]
    }
   ],
   "source": [
    "def normalize(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s.lower().strip())\n",
    "\n",
    "def build_search_space(df: pd.DataFrame):\n",
    "    vocab, key_to_meta = [], {}\n",
    "    for _, row in df.iterrows():\n",
    "        entries = set([row[\"qname\"], row[\"label\"]])\n",
    "        for a in (row[\"alt_labels\"] if isinstance(row[\"alt_labels\"], list) else []):\n",
    "            entries.add(a)\n",
    "        for e in entries:\n",
    "            k = normalize(str(e))\n",
    "            vocab.append(k)\n",
    "            key_to_meta[k] = {\"kind\": row[\"kind\"], \"iri\": row[\"iri\"], \"qname\": row[\"qname\"], \"label\": row[\"label\"]}\n",
    "    return vocab, key_to_meta\n",
    "\n",
    "vocab, key_to_meta = build_search_space(onto_df)\n",
    "print(f\"Search space built with {len(vocab)} surface forms.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ee0283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 220 ontology surface forms with nomic-embed-text ...\n",
      "Ontology embedding index ready: (220, 768)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def _try_embed_batch(texts, model, timeout=120):\n",
    "    \"\"\"Try Ollama embeddings with batched 'input'. Fallback to per-item 'prompt' if needed.\n",
    "    Returns list of vectors (len == len(texts)).\"\"\"\n",
    "    url = \"http://localhost:11434/api/embeddings\"\n",
    "    # Preferred: batched 'input'\n",
    "    try:\n",
    "        r = requests.post(url, json={\"model\": model, \"input\": texts}, timeout=timeout)\n",
    "        if r.ok:\n",
    "            data = r.json()\n",
    "            # Possible shapes:\n",
    "            # 1) { \"embeddings\": [[...], [...], ...] }\n",
    "            # 2) { \"embeddings\": [ { \"embedding\": [...] }, ... ] }\n",
    "            embs = data.get(\"embeddings\")\n",
    "            if isinstance(embs, list) and embs:\n",
    "                if isinstance(embs[0], dict) and \"embedding\" in embs[0]:\n",
    "                    return [e[\"embedding\"] for e in embs]\n",
    "                if isinstance(embs[0], list):\n",
    "                    return embs\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: per-text requests with 'prompt'\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        rr = requests.post(url, json={\"model\": model, \"prompt\": t}, timeout=timeout)\n",
    "        rr.raise_for_status()\n",
    "        dj = rr.json()\n",
    "        vec = dj.get(\"embedding\")\n",
    "        if vec is None:\n",
    "            data = dj.get(\"data\")\n",
    "            if isinstance(data, list) and data and isinstance(data[0], dict):\n",
    "                vec = data[0].get(\"embedding\")\n",
    "        out.append(vec)\n",
    "    return out\n",
    "\n",
    "def embed_texts_ollama(texts, model, batch=64):\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch):\n",
    "        chunk = texts[i:i+batch]\n",
    "        vecs.extend(_try_embed_batch(chunk, model))\n",
    "    return np.array(vecs, dtype=np.float32)\n",
    "\n",
    "def l2_normalize(mat):\n",
    "    mat = np.array(mat, dtype=np.float32)\n",
    "    norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-9\n",
    "    return mat / norms\n",
    "\n",
    "# Build embedding index over ontology surface forms (from Cell 3)\n",
    "if USE_EMBEDDINGS:\n",
    "    print(f\"Embedding {len(vocab)} ontology surface forms with {OLLAMA_EMBED_MODEL} ...\")\n",
    "    E_onto = embed_texts_ollama(vocab, OLLAMA_EMBED_MODEL, batch=EMBED_BATCH)\n",
    "    E_onto = l2_normalize(E_onto)\n",
    "    print(\"Ontology embedding index ready:\", E_onto.shape)\n",
    "else:\n",
    "    E_onto = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da7ec8",
   "metadata": {},
   "source": [
    "Cell 4 — Load Markdown and chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7685e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 22 chunks.\n"
     ]
    }
   ],
   "source": [
    "with open(DOC_MD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    md_text = f.read()\n",
    "\n",
    "CHUNK_CHAR_LIMIT = 2500  # adjust if needed\n",
    "paras = [p.strip() for p in re.split(r\"\\n{2,}\", md_text) if p.strip()]\n",
    "chunks, buf = [], \"\"\n",
    "for p in paras:\n",
    "    if len(buf) + len(p) + 2 <= CHUNK_CHAR_LIMIT:\n",
    "        buf = (buf + \"\\n\\n\" + p).strip()\n",
    "    else:\n",
    "        if buf: chunks.append(buf)\n",
    "        buf = p\n",
    "if buf: chunks.append(buf)\n",
    "\n",
    "print(f\"Document split into {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33923693",
   "metadata": {},
   "source": [
    "Cell 5 — Ollama helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa544d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama helper ready.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def ollama_chat(model: str, system_prompt: str, user_prompt: str, as_json: bool = True, timeout: int = 120):\n",
    "    \"\"\"Calls Ollama's /api/chat endpoint. Ensure Ollama is running locally.\"\"\"\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    hdrs = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        \"options\": {\"temperature\": 0.2}\n",
    "    }\n",
    "    r = requests.post(url, headers=hdrs, json=data, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    content = r.json().get(\"message\", {}).get(\"content\", \"\")\n",
    "    if as_json:\n",
    "        m = re.search(r\"\\{.*\\}\", content, flags=re.DOTALL)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return content\n",
    "\n",
    "print(\"Ollama helper ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35572c45",
   "metadata": {},
   "source": [
    "Cell 6 — Mapping prompts (Flat JSON default, JSON-LD optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c95d4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt templates ready.\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert ontology mapper. Given an ontology dictionary and a text chunk, \"\n",
    "    \"extract: (A) mentions aligned to existing ontology classes/properties, and (B) new concept candidates. \"\n",
    "    \"Return the requested output format exactly.\"\n",
    ")\n",
    "\n",
    "def make_user_prompt(chunk_text: str, ontology_terms: pd.DataFrame, format: str = \"flat_json\") -> str:\n",
    "    MAX_TERMS = 300\n",
    "    df = ontology_terms.head(MAX_TERMS)\n",
    "\n",
    "    lines = []\n",
    "    for _, r in df.iterrows():\n",
    "        alts = \", \".join(r[\"alt_labels\"]) if isinstance(r[\"alt_labels\"], list) else \"\"\n",
    "        base = f\"- [{r['kind']}] {r['qname']} | label: {r['label']}\"\n",
    "        lines.append(base + (f\" | alt: {alts}\" if alts else \"\"))\n",
    "\n",
    "    if format == \"flat_json\":\n",
    "        out_spec = \"\"\"\n",
    "Return strictly valid JSON with two arrays:\n",
    "{\n",
    "  \"mapped\": [\n",
    "    {\n",
    "      \"span\": \"<exact text>\",\n",
    "      \"sentence\": \"<the sentence>\",\n",
    "      \"concept_kind\": \"class|object_property|data_property|instance\",\n",
    "      \"ontology_term\": \"<qname or iri>\",\n",
    "      \"confidence\": 0-1\n",
    "    }\n",
    "  ],\n",
    "  \"new_candidates\": [\n",
    "    {\n",
    "      \"term\": \"<preferred label>\",\n",
    "      \"type_guess\": \"class|object_property|data_property\",\n",
    "      \"definition\": \"<one-sentence definition inferred from text>\",\n",
    "      \"example_sentence\": \"<verbatim sentence from chunk>\",\n",
    "      \"rationale\": \"<why it seems new>\",\n",
    "      \"confidence\": 0-1\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "    else:\n",
    "        out_spec = \"\"\"\n",
    "Return strictly valid JSON-LD with an @graph array; also add top-level \"new_candidates\".\n",
    "Use @type for class and properties as keys; keep nesting minimal.\n",
    "{\n",
    "  \"@graph\": [ { \"@type\": \"ex:YourClass\", \"ex:someProperty\": \"value\" } ],\n",
    "  \"new_candidates\": [ { \"term\": \"...\", \"type_guess\": \"...\", \"definition\": \"...\", \"example_sentence\": \"...\", \"confidence\": 0-1 } ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"ONTOLOGY DICTIONARY (subset):\n",
    "{chr(10).join(lines)}\n",
    "\n",
    "TEXT CHUNK (verbatim):\n",
    "<<<\n",
    "{chunk_text}\n",
    "<<<\n",
    "\n",
    "OUTPUT FORMAT SPEC:\n",
    "{out_spec}\n",
    "\n",
    "Rules:\n",
    "- Use only terms that appear in the dictionary for \"ontology_term\".\n",
    "- If no matching term exists for a mention, put it under \"new_candidates\".\n",
    "- Be conservative; prefer precision over recall.\n",
    "- Ensure the JSON is syntactically valid.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "print(\"Prompt templates ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8525f1",
   "metadata": {},
   "source": [
    "Cell 7 — Run mapping over chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a6e2e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/22 ...\n",
      "Processing chunk 2/22 ...\n",
      "Processing chunk 3/22 ...\n",
      "Processing chunk 4/22 ...\n",
      "Processing chunk 5/22 ...\n",
      "Processing chunk 6/22 ...\n",
      "Processing chunk 7/22 ...\n",
      "Processing chunk 8/22 ...\n",
      "Processing chunk 9/22 ...\n",
      "Processing chunk 10/22 ...\n",
      "Processing chunk 11/22 ...\n",
      "Processing chunk 12/22 ...\n",
      "Processing chunk 13/22 ...\n",
      "Processing chunk 14/22 ...\n",
      "Processing chunk 15/22 ...\n",
      "Processing chunk 16/22 ...\n",
      "Processing chunk 17/22 ...\n",
      "Processing chunk 18/22 ...\n",
      "Processing chunk 19/22 ...\n",
      "Processing chunk 20/22 ...\n",
      "Processing chunk 21/22 ...\n",
      "Processing chunk 22/22 ...\n",
      "Collected 163 mapped mentions and 54 new concept candidates.\n"
     ]
    }
   ],
   "source": [
    "all_mapped, all_new = [], []\n",
    "fmt = \"flat_json\" if MAPPING_FORMAT.lower() == \"flat_json\" else \"jsonld\"\n",
    "\n",
    "for i, ch in enumerate(chunks, 1):\n",
    "    print(f\"Processing chunk {i}/{len(chunks)} ...\")\n",
    "    up = make_user_prompt(ch, onto_df, format=fmt)\n",
    "    resp = ollama_chat(OLLAMA_MODEL, SYSTEM_PROMPT, up, as_json=True)\n",
    "\n",
    "    if isinstance(resp, dict) and \"mapped\" in resp:  # flat_json path\n",
    "        mapped = resp.get(\"mapped\", [])\n",
    "        # add the current chunk id to every mapped row (flat branch)\n",
    "        for r in mapped:\n",
    "            r[\"chunk_id\"] = i\n",
    "        newc = resp.get(\"new_candidates\", [])\n",
    "\n",
    "    else:  # jsonld path → flatten a bit\n",
    "        mapped = resp.get(\"@graph\", []) if isinstance(resp, dict) else []\n",
    "        newc = resp.get(\"new_candidates\", []) if isinstance(resp, dict) else []\n",
    "        flat_rows = []\n",
    "        for item in mapped:\n",
    "            cls = item.get(\"@type\", \"\")\n",
    "            for k, v in item.items():\n",
    "                if k.startswith(\"@\"):\n",
    "                    continue\n",
    "                if isinstance(v, list):\n",
    "                    for el in v:\n",
    "                        flat_rows.append({\n",
    "                            \"span\": str(el),\n",
    "                            \"sentence\": \"\",\n",
    "                            \"concept_kind\": \"object_property\",\n",
    "                            \"ontology_term\": k,\n",
    "                            \"confidence\": 0.6\n",
    "                        })\n",
    "                else:\n",
    "                    flat_rows.append({\n",
    "                        \"span\": str(v),\n",
    "                        \"sentence\": \"\",\n",
    "                        \"concept_kind\": \"object_property\",\n",
    "                        \"ontology_term\": k,\n",
    "                        \"confidence\": 0.6\n",
    "                    })\n",
    "            if cls:\n",
    "                flat_rows.append({\n",
    "                    \"span\": str(cls),\n",
    "                    \"sentence\": \"\",\n",
    "                    \"concept_kind\": \"class\",\n",
    "                    \"ontology_term\": str(cls),\n",
    "                    \"confidence\": 0.6\n",
    "                })\n",
    "        mapped = flat_rows\n",
    "        # add the current chunk id to every mapped row (jsonld branch)\n",
    "        for r in mapped:\n",
    "            r[\"chunk_id\"] = i\n",
    "\n",
    "    all_mapped.extend(mapped)\n",
    "    all_new.extend(newc)\n",
    "\n",
    "print(f\"Collected {len(all_mapped)} mapped mentions and {len(all_new)} new concept candidates.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab63a82d",
   "metadata": {},
   "source": [
    "Cell 8 — Post-process (align & filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2131659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned mapped mentions: 53\n",
      "New concept candidates: 54\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>sentence</th>\n",
       "      <th>concept_kind</th>\n",
       "      <th>ontology_term</th>\n",
       "      <th>confidence</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>span_norm</th>\n",
       "      <th>match_meta</th>\n",
       "      <th>matched_qname</th>\n",
       "      <th>matched_kind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Operational Equipment Effectiveness (OEE)</td>\n",
       "      <td>Packaging material production lines are genera...</td>\n",
       "      <td>class</td>\n",
       "      <td>esco:Performance</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>operational equipment effectiveness (oee)</td>\n",
       "      <td>{'qname': ':Operational', 'kind': 'class'}</td>\n",
       "      <td>:Operational</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>production halts</td>\n",
       "      <td>Consequently, this leads to inconsistent produ...</td>\n",
       "      <td>class</td>\n",
       "      <td>:Reliability</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>production halts</td>\n",
       "      <td>{'qname': ':Reliability', 'kind': 'class'}</td>\n",
       "      <td>:Reliability</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>less skilled, inexperienced operators</td>\n",
       "      <td>Given the repetitive and demanding nature of m...</td>\n",
       "      <td>instance</td>\n",
       "      <td>esco:HumanActor</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>less skilled, inexperienced operators</td>\n",
       "      <td>{'qname': 'esco:Skill', 'kind': 'class'}</td>\n",
       "      <td>esco:Skill</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AI-powered digital production assistant</td>\n",
       "      <td>The manufacturing pilot aims to address this i...</td>\n",
       "      <td>instance</td>\n",
       "      <td>:DigitalTwinDecision</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>ai-powered digital production assistant</td>\n",
       "      <td>{'qname': ':DigitalTwinDecision', 'kind': 'cla...</td>\n",
       "      <td>:DigitalTwinDecision</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>capturing knowledge from experienced workers</td>\n",
       "      <td>The AI-powered digital production assistant wi...</td>\n",
       "      <td>object_property</td>\n",
       "      <td>:hasSkill</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>capturing knowledge from experienced workers</td>\n",
       "      <td>{'qname': ':hasSkill', 'kind': 'object_property'}</td>\n",
       "      <td>:hasSkill</td>\n",
       "      <td>object_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>determining how to effectively and legally mon...</td>\n",
       "      <td>The AI-powered digital production assistant wi...</td>\n",
       "      <td>object_property</td>\n",
       "      <td>:monitorsActor</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>determining how to effectively and legally mon...</td>\n",
       "      <td>{'qname': ':monitorsActor', 'kind': 'object_pr...</td>\n",
       "      <td>:monitorsActor</td>\n",
       "      <td>object_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>possible methods include recording via cameras...</td>\n",
       "      <td>Possible methods include recording via cameras...</td>\n",
       "      <td>object_property</td>\n",
       "      <td>:usesResource</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>possible methods include recording via cameras...</td>\n",
       "      <td>{'qname': ':usesResource', 'kind': 'object_pro...</td>\n",
       "      <td>:usesResource</td>\n",
       "      <td>object_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Link specific maintenance and repair operation...</td>\n",
       "      <td>The AI-powered digital production assistant wi...</td>\n",
       "      <td>object_property</td>\n",
       "      <td>:taskHasConstraint</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>link specific maintenance and repair operation...</td>\n",
       "      <td>{'qname': ':taskHasConstraint', 'kind': 'objec...</td>\n",
       "      <td>:taskHasConstraint</td>\n",
       "      <td>object_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Provide targeted instructions to operators</td>\n",
       "      <td>The AI-powered digital production assistant wi...</td>\n",
       "      <td>object_property</td>\n",
       "      <td>:assignedTo</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>provide targeted instructions to operators</td>\n",
       "      <td>{'qname': ':assignedTo', 'kind': 'object_prope...</td>\n",
       "      <td>:assignedTo</td>\n",
       "      <td>object_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Collect feedback from users and adapt based on...</td>\n",
       "      <td>The AI-powered digital production assistant wi...</td>\n",
       "      <td>object_property</td>\n",
       "      <td>:hasActorStatus</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>collect feedback from users and adapt based on...</td>\n",
       "      <td>{'qname': ':hasActorStatus', 'kind': 'object_p...</td>\n",
       "      <td>:hasActorStatus</td>\n",
       "      <td>object_property</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 span  \\\n",
       "1           Operational Equipment Effectiveness (OEE)   \n",
       "5                                    production halts   \n",
       "7               less skilled, inexperienced operators   \n",
       "8             AI-powered digital production assistant   \n",
       "9        capturing knowledge from experienced workers   \n",
       "10  determining how to effectively and legally mon...   \n",
       "11  possible methods include recording via cameras...   \n",
       "12  Link specific maintenance and repair operation...   \n",
       "13         Provide targeted instructions to operators   \n",
       "14  Collect feedback from users and adapt based on...   \n",
       "\n",
       "                                             sentence     concept_kind  \\\n",
       "1   Packaging material production lines are genera...            class   \n",
       "5   Consequently, this leads to inconsistent produ...            class   \n",
       "7   Given the repetitive and demanding nature of m...         instance   \n",
       "8   The manufacturing pilot aims to address this i...         instance   \n",
       "9   The AI-powered digital production assistant wi...  object_property   \n",
       "10  The AI-powered digital production assistant wi...  object_property   \n",
       "11  Possible methods include recording via cameras...  object_property   \n",
       "12  The AI-powered digital production assistant wi...  object_property   \n",
       "13  The AI-powered digital production assistant wi...  object_property   \n",
       "14  The AI-powered digital production assistant wi...  object_property   \n",
       "\n",
       "           ontology_term  confidence  chunk_id  \\\n",
       "1       esco:Performance         0.8         2   \n",
       "5           :Reliability         0.6         2   \n",
       "7        esco:HumanActor         0.6         2   \n",
       "8   :DigitalTwinDecision         0.5         2   \n",
       "9              :hasSkill         0.5         2   \n",
       "10        :monitorsActor         0.4         2   \n",
       "11         :usesResource         0.4         2   \n",
       "12    :taskHasConstraint         0.4         2   \n",
       "13           :assignedTo         0.4         2   \n",
       "14       :hasActorStatus         0.4         2   \n",
       "\n",
       "                                            span_norm  \\\n",
       "1           operational equipment effectiveness (oee)   \n",
       "5                                    production halts   \n",
       "7               less skilled, inexperienced operators   \n",
       "8             ai-powered digital production assistant   \n",
       "9        capturing knowledge from experienced workers   \n",
       "10  determining how to effectively and legally mon...   \n",
       "11  possible methods include recording via cameras...   \n",
       "12  link specific maintenance and repair operation...   \n",
       "13         provide targeted instructions to operators   \n",
       "14  collect feedback from users and adapt based on...   \n",
       "\n",
       "                                           match_meta         matched_qname  \\\n",
       "1          {'qname': ':Operational', 'kind': 'class'}          :Operational   \n",
       "5          {'qname': ':Reliability', 'kind': 'class'}          :Reliability   \n",
       "7            {'qname': 'esco:Skill', 'kind': 'class'}            esco:Skill   \n",
       "8   {'qname': ':DigitalTwinDecision', 'kind': 'cla...  :DigitalTwinDecision   \n",
       "9   {'qname': ':hasSkill', 'kind': 'object_property'}             :hasSkill   \n",
       "10  {'qname': ':monitorsActor', 'kind': 'object_pr...        :monitorsActor   \n",
       "11  {'qname': ':usesResource', 'kind': 'object_pro...         :usesResource   \n",
       "12  {'qname': ':taskHasConstraint', 'kind': 'objec...    :taskHasConstraint   \n",
       "13  {'qname': ':assignedTo', 'kind': 'object_prope...           :assignedTo   \n",
       "14  {'qname': ':hasActorStatus', 'kind': 'object_p...       :hasActorStatus   \n",
       "\n",
       "       matched_kind  \n",
       "1             class  \n",
       "5             class  \n",
       "7             class  \n",
       "8             class  \n",
       "9   object_property  \n",
       "10  object_property  \n",
       "11  object_property  \n",
       "12  object_property  \n",
       "13  object_property  \n",
       "14  object_property  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_df = pd.DataFrame(all_mapped)\n",
    "if not mapped_df.empty:\n",
    "    mapped_df[\"span_norm\"] = mapped_df[\"span\"].astype(str).str.lower().str.strip()\n",
    "    known_qnames = set(onto_df[\"qname\"].str.lower())\n",
    "    known_iris = set(onto_df[\"iri\"].str.lower())\n",
    "\n",
    "    def pick_meta(term: str, span_text: str):\n",
    "        \"\"\"Map to ontology via exact → embedding → fuzzy.\"\"\"\n",
    "        # 1) exact qname/IRI\n",
    "        if term:\n",
    "            k = str(term).lower().strip()\n",
    "            if k in known_qnames or k in known_iris:\n",
    "                return {\"qname\": term, \"kind\": key_to_meta.get(k, {}).get(\"kind\", \"\")}\n",
    "\n",
    "        # 2) embeddings (prefer span text)\n",
    "        if USE_EMBEDDINGS and E_onto is not None:\n",
    "            qtxt = (span_text or term or \"\").strip()\n",
    "            if qtxt:\n",
    "                qv = embed_texts_ollama([qtxt], OLLAMA_EMBED_MODEL, batch=1)\n",
    "                qv = l2_normalize(qv)[0]\n",
    "                sims = E_onto @ qv\n",
    "                top_idx = sims.argsort()[-EMBED_TOPK:][::-1]\n",
    "                best_idx = int(top_idx[0])\n",
    "                best_key = vocab[best_idx]\n",
    "                best_sim = float(sims[best_idx])\n",
    "\n",
    "                # Blend with fuzzy for stability\n",
    "                from rapidfuzz import process, fuzz\n",
    "                fuzzy_match = process.extractOne(qtxt.lower(), list(key_to_meta.keys()), scorer=fuzz.WRatio)\n",
    "                fuzzy_score = (fuzzy_match[1] / 100.0) if fuzzy_match else 0.0\n",
    "\n",
    "                blended = COMBO_ALPHA * best_sim + (1.0 - COMBO_ALPHA) * fuzzy_score\n",
    "                if (best_sim >= EMBED_MIN) or (blended >= COMBO_MIN):\n",
    "                    meta = key_to_meta.get(best_key)\n",
    "                    if meta:\n",
    "                        return {\"qname\": meta[\"qname\"], \"kind\": meta[\"kind\"]}\n",
    "\n",
    "        # 3) fuzzy fallback\n",
    "        if key_to_meta:\n",
    "            q = (span_text or term or \"\").lower().strip()\n",
    "            cand = process.extractOne(q, list(key_to_meta.keys()), scorer=fuzz.WRatio)\n",
    "            if cand and cand[1] >= 90:\n",
    "                meta = key_to_meta[cand[0]]\n",
    "                return {\"qname\": meta[\"qname\"], \"kind\": meta[\"kind\"]}\n",
    "        return None\n",
    "\n",
    "    metas = [pick_meta(row.get(\"ontology_term\",\"\"), row.get(\"span\",\"\")) for _, row in mapped_df.iterrows()]\n",
    "    mapped_df[\"match_meta\"] = metas\n",
    "    mapped_df = mapped_df[mapped_df[\"match_meta\"].notnull()]\n",
    "    mapped_df[\"matched_qname\"] = mapped_df[\"match_meta\"].apply(lambda m: m[\"qname\"] if m else \"\")\n",
    "    mapped_df[\"matched_kind\"] = mapped_df[\"match_meta\"].apply(lambda m: m[\"kind\"] if m else \"\")\n",
    "else:\n",
    "    mapped_df = pd.DataFrame(columns=[\n",
    "        \"span\",\"sentence\",\"concept_kind\",\"ontology_term\",\"confidence\",\"span_norm\",\"match_meta\",\"matched_qname\",\"matched_kind\"\n",
    "    ])\n",
    "\n",
    "new_df = pd.DataFrame(all_new).fillna(\"\")\n",
    "\n",
    "print(\"Aligned mapped mentions:\", len(mapped_df))\n",
    "print(\"New concept candidates:\", len(new_df))\n",
    "mapped_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built hyperedges for 15 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Build per-chunk “hyperedges” from the current matches\n",
    "# Each chunk edge stores the classes and properties matched in that chunk\n",
    "edges = {}  # chunk_id -> {\"classes\": set(), \"props\": set(), \"counts\": Counter()}\n",
    "from collections import Counter\n",
    "\n",
    "for _, r in mapped_df.iterrows():\n",
    "    cid = int(r.get(\"chunk_id\", -1))\n",
    "    edges.setdefault(cid, {\"classes\": set(), \"props\": set(), \"counts\": Counter()})\n",
    "    qn = r.get(\"matched_qname\",\"\")\n",
    "    k  = r.get(\"matched_kind\",\"\")\n",
    "    if qn:\n",
    "        edges[cid][\"counts\"][qn] += 1\n",
    "    if k == \"class\" and qn:\n",
    "        edges[cid][\"classes\"].add(qn)\n",
    "    elif (k == \"object_property\" or k == \"data_property\") and qn:\n",
    "        edges[cid][\"props\"].add(qn)\n",
    "\n",
    "print(f\"Built hyperedges for {len(edges)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HG re-scoring done. Accepted (hg): 29 / 29\n"
     ]
    }
   ],
   "source": [
    "# Hypergraph-aware rescoring\n",
    "USE_HYPERGRAPH_RESCORING = True\n",
    "SCHEMA_BONUS   = 0.08  # domain compatibility boost\n",
    "COHESION_BONUS = 0.05  # repeated same-term boost within a chunk\n",
    "SCHEMA_PENALTY = 0.07  # properties present but no compatible class in chunk\n",
    "ACCEPT_RESCORE = 0.78  # optional acceptance threshold\n",
    "\n",
    "def hg_rescore_row(row) -> float:\n",
    "    s0 = float(row.get(\"confidence\", 0.78))\n",
    "    cid = int(row.get(\"chunk_id\", -1))\n",
    "    qn  = row.get(\"matched_qname\",\"\")\n",
    "    kind= row.get(\"matched_kind\",\"\")\n",
    "    if cid not in edges or not qn:\n",
    "        return s0\n",
    "\n",
    "    cls_in_chunk = edges[cid][\"classes\"]\n",
    "    counts = edges[cid][\"counts\"]\n",
    "    s = s0\n",
    "\n",
    "    # Cohesion: multiple mentions of the same term in the chunk\n",
    "    if counts.get(qn, 0) >= 2:\n",
    "        s += COHESION_BONUS\n",
    "\n",
    "    # Schema support: properties get bonus if a compatible class is in the same chunk\n",
    "    if kind in (\"object_property\",\"data_property\"):\n",
    "        doms = prop_sig.get(qn, {}).get(\"domain\", {\"owl:Thing\"})\n",
    "        if doms != {\"owl:Thing\"}:\n",
    "            if any(domain_matches(qn, c) for c in cls_in_chunk):\n",
    "                s += SCHEMA_BONUS\n",
    "            elif cls_in_chunk:\n",
    "                s -= SCHEMA_PENALTY\n",
    "\n",
    "    # Classes get a small bonus if some property in the chunk expects them (or a superclass)\n",
    "    if kind == \"class\":\n",
    "        props = edges[cid][\"props\"]\n",
    "        for p in props:\n",
    "            if domain_matches(p, qn):\n",
    "                s += SCHEMA_BONUS * 0.6\n",
    "                break\n",
    "\n",
    "    return max(0.0, min(1.0, s))\n",
    "\n",
    "if USE_HYPERGRAPH_RESCORING and not mapped_df.empty:\n",
    "    mapped_df[\"hg_score\"]  = mapped_df.apply(hg_rescore_row, axis=1)\n",
    "    mapped_df[\"hg_accept\"] = mapped_df[\"hg_score\"] >= ACCEPT_RESCORE\n",
    "    print(\"HG re-scoring done. Accepted (hg):\", int(mapped_df[\"hg_accept\"].sum()), \"/\", len(mapped_df))\n",
    "    # OPTIONAL strict filter:\n",
    "    # mapped_df = mapped_df[mapped_df[\"hg_accept\"]].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e25b115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ./outputs\\dso_outline.md\n",
      "Also wrote: ./outputs\\mapped_mentions.csv and ./outputs\\new_concepts.csv\n"
     ]
    }
   ],
   "source": [
    "used_classes = sorted(set(mapped_df.loc[mapped_df[\"matched_kind\"]==\"class\", \"matched_qname\"].tolist()))\n",
    "used_obj_props = sorted(set(mapped_df.loc[mapped_df[\"matched_kind\"]==\"object_property\", \"matched_qname\"].tolist()))\n",
    "used_data_props = sorted(set(mapped_df.loc[mapped_df[\"matched_kind\"]==\"data_property\", \"matched_qname\"].tolist()))\n",
    "\n",
    "def group_candidates(df: pd.DataFrame):\n",
    "    groups = {}\n",
    "    if df.empty:\n",
    "        return groups\n",
    "    for tg in [\"class\",\"object_property\",\"data_property\",\"\"]:\n",
    "        sub = df[df[\"type_guess\"].str.lower()==tg] if tg else df[df[\"type_guess\"]==\"\"]\n",
    "        if not sub.empty:\n",
    "            items = []\n",
    "            for _, r in sub.iterrows():\n",
    "                items.append({\n",
    "                    \"term\": r.get(\"term\",\"\"),\n",
    "                    \"definition\": r.get(\"definition\",\"\"),\n",
    "                    \"example_sentence\": r.get(\"example_sentence\",\"\"),\n",
    "                    \"rationale\": r.get(\"rationale\",\"\"),\n",
    "                    \"confidence\": r.get(\"confidence\",\"\")\n",
    "                })\n",
    "            groups[tg if tg else \"unspecified\"] = items\n",
    "    return groups\n",
    "\n",
    "groups = group_candidates(new_df)\n",
    "\n",
    "dso_lines = []\n",
    "dso_lines.append(\"# Domain-Specific Ontology (Text Outline)\\n\")\n",
    "dso_lines.append(\"## Reused Classes\")\n",
    "dso_lines += [f\"- {c}\" for c in used_classes] if used_classes else [\"- (none)\"]\n",
    "\n",
    "dso_lines.append(\"\\n## Reused Object Properties\")\n",
    "dso_lines += [f\"- {p}\" for p in used_obj_props] if used_obj_props else [\"- (none)\"]\n",
    "\n",
    "dso_lines.append(\"\\n## Reused Data Properties\")\n",
    "dso_lines += [f\"- {p}\" for p in used_data_props] if used_data_props else [\"- (none)\"]\n",
    "\n",
    "dso_lines.append(\"\\n## New Class Candidates\")\n",
    "for item in groups.get(\"class\", []):\n",
    "    dso_lines.append(f\"- **{item['term']}** — {item['definition']}  (e.g., \\\"{item['example_sentence']}\\\")\")\n",
    "\n",
    "dso_lines.append(\"\\n## New Object Property Candidates\")\n",
    "for item in groups.get(\"object_property\", []):\n",
    "    dso_lines.append(f\"- **{item['term']}** — {item['definition']}  (e.g., \\\"{item['example_sentence']}\\\")\")\n",
    "\n",
    "dso_lines.append(\"\\n## New Data Property Candidates\")\n",
    "for item in groups.get(\"data_property\", []):\n",
    "    dso_lines.append(f\"- **{item['term']}** — {item['definition']}  (e.g., \\\"{item['example_sentence']}\\\")\")\n",
    "\n",
    "dso_lines.append(\"\\n## Unspecified-Type Candidates\")\n",
    "for item in groups.get(\"unspecified\", []):\n",
    "    dso_lines.append(f\"- **{item['term']}** — {item['definition']}  (e.g., \\\"{item['example_sentence']}\\\")\")\n",
    "\n",
    "outline_path = os.path.join(OUT_DIR, \"dso_outline.md\")\n",
    "with open(outline_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(dso_lines))\n",
    "\n",
    "mapped_csv = os.path.join(OUT_DIR, \"mapped_mentions.csv\")\n",
    "new_csv = os.path.join(OUT_DIR, \"new_concepts.csv\")\n",
    "mapped_df.to_csv(mapped_csv, index=False, encoding=\"utf-8\")\n",
    "new_df.to_csv(new_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Wrote:\", outline_path)\n",
    "print(\"Also wrote:\", mapped_csv, \"and\", new_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
